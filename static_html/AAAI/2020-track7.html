<!DOCTYPE html>
<html lang="en-US">
<head >
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name='robots' content='index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1' />
	<style>img:is([sizes="auto" i], [sizes^="auto," i]) { contain-intrinsic-size: 3000px 1500px }</style>
	
	<!-- This site is optimized with the Yoast SEO plugin v23.4 - https://yoast.com/wordpress/plugins/seo/ -->
	<title>Vol. 34 No. 07: AAAI-20 Technical Tracks 7 Archives - AAAI</title>
	<link rel="canonical" href="https://aaai.org/proceeding/vol-34-no-07-aaai-20-technical-tracks-7/" />
	<link rel="next" href="https://aaai.org/proceeding/vol-34-no-07-aaai-20-technical-tracks-7/page/2/" />
	<meta property="og:locale" content="en_US" />
	<meta property="og:type" content="article" />
	<meta property="og:title" content="Vol. 34 No. 07: AAAI-20 Technical Tracks 7 Archives - AAAI" />
	<meta property="og:url" content="https://aaai.org/proceeding/vol-34-no-07-aaai-20-technical-tracks-7/" />
	<meta property="og:site_name" content="AAAI" />
	<meta name="twitter:card" content="summary_large_image" />
	<script type="application/ld+json" class="yoast-schema-graph">{"@context":"https://schema.org","@graph":[{"@type":"CollectionPage","@id":"https://aaai.org/proceeding/vol-34-no-07-aaai-20-technical-tracks-7/","url":"https://aaai.org/proceeding/vol-34-no-07-aaai-20-technical-tracks-7/","name":"Vol. 34 No. 07: AAAI-20 Technical Tracks 7 Archives - AAAI","isPartOf":{"@id":"https://aaai.org/#website"},"breadcrumb":{"@id":"https://aaai.org/proceeding/vol-34-no-07-aaai-20-technical-tracks-7/#breadcrumb"},"inLanguage":"en-US"},{"@type":"BreadcrumbList","@id":"https://aaai.org/proceeding/vol-34-no-07-aaai-20-technical-tracks-7/#breadcrumb","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https://aaai.org/"},{"@type":"ListItem","position":2,"name":"Papers","item":"https://aaai.org/papers/"},{"@type":"ListItem","position":3,"name":"Proceedings of the AAAI Conference on Artificial Intelligence, 34","item":"https://aaai.org/proceeding/aaai-34-2020/"},{"@type":"ListItem","position":4,"name":"Vol. 34 No. 07: AAAI-20 Technical Tracks 7"}]},{"@type":"WebSite","@id":"https://aaai.org/#website","url":"https://aaai.org/","name":"AAAI","description":"Association for the Advancement of Artificial Intelligence","publisher":{"@id":"https://aaai.org/#organization"},"potentialAction":[{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://aaai.org/?s={search_term_string}"},"query-input":{"@type":"PropertyValueSpecification","valueRequired":true,"valueName":"search_term_string"}}],"inLanguage":"en-US"},{"@type":"Organization","@id":"https://aaai.org/#organization","name":"AAAI","url":"https://aaai.org/","logo":{"@type":"ImageObject","inLanguage":"en-US","@id":"https://aaai.org/#/schema/logo/image/","url":"https://aaai.org/wp-content/uploads/2023/02/aaai-logo-RGB.jpg","contentUrl":"https://aaai.org/wp-content/uploads/2023/02/aaai-logo-RGB.jpg","width":1051,"height":750,"caption":"AAAI"},"image":{"@id":"https://aaai.org/#/schema/logo/image/"}}]}</script>
	<!-- / Yoast SEO plugin. -->


<link rel='dns-prefetch' href='//fonts.googleapis.com' />
<link href='https://fonts.gstatic.com' crossorigin rel='preconnect' />
<link rel="alternate" type="application/rss+xml" title="AAAI &raquo; Feed" href="https://aaai.org/feed/" />
<link rel="alternate" type="application/rss+xml" title="AAAI &raquo; Comments Feed" href="https://aaai.org/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="AAAI &raquo; Vol. 34 No. 07: AAAI-20 Technical Tracks 7 Proceeding Feed" href="https://aaai.org/proceeding/vol-34-no-07-aaai-20-technical-tracks-7/feed/" />
<script>
window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.0.3\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.0.3\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/aaai.org\/wp-includes\/js\/wp-emoji-release.min.js?ver=6.7.1"}};
/*! This file is auto-generated */
!function(i,n){var o,s,e;function c(e){try{var t={supportTests:e,timestamp:(new Date).valueOf()};sessionStorage.setItem(o,JSON.stringify(t))}catch(e){}}function p(e,t,n){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);var t=new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data),r=(e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(n,0,0),new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data));return t.every(function(e,t){return e===r[t]})}function u(e,t,n){switch(t){case"flag":return n(e,"\ud83c\udff3\ufe0f\u200d\u26a7\ufe0f","\ud83c\udff3\ufe0f\u200b\u26a7\ufe0f")?!1:!n(e,"\ud83c\uddfa\ud83c\uddf3","\ud83c\uddfa\u200b\ud83c\uddf3")&&!n(e,"\ud83c\udff4\udb40\udc67\udb40\udc62\udb40\udc65\udb40\udc6e\udb40\udc67\udb40\udc7f","\ud83c\udff4\u200b\udb40\udc67\u200b\udb40\udc62\u200b\udb40\udc65\u200b\udb40\udc6e\u200b\udb40\udc67\u200b\udb40\udc7f");case"emoji":return!n(e,"\ud83d\udc26\u200d\u2b1b","\ud83d\udc26\u200b\u2b1b")}return!1}function f(e,t,n){var r="undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?new OffscreenCanvas(300,150):i.createElement("canvas"),a=r.getContext("2d",{willReadFrequently:!0}),o=(a.textBaseline="top",a.font="600 32px Arial",{});return e.forEach(function(e){o[e]=t(a,e,n)}),o}function t(e){var t=i.createElement("script");t.src=e,t.defer=!0,i.head.appendChild(t)}"undefined"!=typeof Promise&&(o="wpEmojiSettingsSupports",s=["flag","emoji"],n.supports={everything:!0,everythingExceptFlag:!0},e=new Promise(function(e){i.addEventListener("DOMContentLoaded",e,{once:!0})}),new Promise(function(t){var n=function(){try{var e=JSON.parse(sessionStorage.getItem(o));if("object"==typeof e&&"number"==typeof e.timestamp&&(new Date).valueOf()<e.timestamp+604800&&"object"==typeof e.supportTests)return e.supportTests}catch(e){}return null}();if(!n){if("undefined"!=typeof Worker&&"undefined"!=typeof OffscreenCanvas&&"undefined"!=typeof URL&&URL.createObjectURL&&"undefined"!=typeof Blob)try{var e="postMessage("+f.toString()+"("+[JSON.stringify(s),u.toString(),p.toString()].join(",")+"));",r=new Blob([e],{type:"text/javascript"}),a=new Worker(URL.createObjectURL(r),{name:"wpTestEmojiSupports"});return void(a.onmessage=function(e){c(n=e.data),a.terminate(),t(n)})}catch(e){}c(n=f(s,u,p))}t(n)}).then(function(e){for(var t in e)n.supports[t]=e[t],n.supports.everything=n.supports.everything&&n.supports[t],"flag"!==t&&(n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&n.supports[t]);n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&!n.supports.flag,n.DOMReady=!1,n.readyCallback=function(){n.DOMReady=!0}}).then(function(){return e}).then(function(){var e;n.supports.everything||(n.readyCallback(),(e=n.source||{}).concatemoji?t(e.concatemoji):e.wpemoji&&e.twemoji&&(t(e.twemoji),t(e.wpemoji)))}))}((window,document),window._wpemojiSettings);
</script>
<link rel='stylesheet' id='genesis-blocks-style-css-css' href='https://aaai.org/wp-content/plugins/genesis-blocks/dist/style-blocks.build.css?ver=1738785529' media='all' />
<link rel='stylesheet' id='aaai-genesis-child-theme-css' href='https://aaai.org/wp-content/themes/genesis-sample/style.css?ver=3.4.1' media='all' />
<style id='aaai-genesis-child-theme-inline-css'>

		.wp-custom-logo .site-container .custom-logo-link {
			aspect-ratio: 180/42.3;
		}
		
		.wp-custom-logo .site-container .title-area {
			max-width: 180px;
		}
		
		.wp-custom-logo .title-area {
			padding-top: 13.85px;
		}
		
</style>
<style id='wp-emoji-styles-inline-css'>

	img.wp-smiley, img.emoji {
		display: inline !important;
		border: none !important;
		box-shadow: none !important;
		height: 1em !important;
		width: 1em !important;
		margin: 0 0.07em !important;
		vertical-align: -0.1em !important;
		background: none !important;
		padding: 0 !important;
	}
</style>
<link rel='stylesheet' id='wp-block-library-css' href='https://aaai.org/wp-includes/css/dist/block-library/style.min.css?ver=6.7.1' media='all' />
<style id='outermost-icon-block-style-inline-css'>
.wp-block-outermost-icon-block{display:flex;line-height:0}.wp-block-outermost-icon-block.has-border-color{border:none}.wp-block-outermost-icon-block .has-icon-color svg,.wp-block-outermost-icon-block.has-icon-color svg{color:currentColor}.wp-block-outermost-icon-block .has-icon-color:not(.has-no-icon-fill-color) svg,.wp-block-outermost-icon-block.has-icon-color:not(.has-no-icon-fill-color) svg{fill:currentColor}.wp-block-outermost-icon-block .icon-container{box-sizing:border-box}.wp-block-outermost-icon-block a,.wp-block-outermost-icon-block svg{height:100%;transition:transform .1s ease-in-out;width:100%}.wp-block-outermost-icon-block a:hover{transform:scale(1.1)}.wp-block-outermost-icon-block svg{transform:rotate(var(--outermost--icon-block--transform-rotate,0deg)) scaleX(var(--outermost--icon-block--transform-scale-x,1)) scaleY(var(--outermost--icon-block--transform-scale-y,1))}.wp-block-outermost-icon-block .rotate-90,.wp-block-outermost-icon-block.rotate-90{--outermost--icon-block--transform-rotate:90deg}.wp-block-outermost-icon-block .rotate-180,.wp-block-outermost-icon-block.rotate-180{--outermost--icon-block--transform-rotate:180deg}.wp-block-outermost-icon-block .rotate-270,.wp-block-outermost-icon-block.rotate-270{--outermost--icon-block--transform-rotate:270deg}.wp-block-outermost-icon-block .flip-horizontal,.wp-block-outermost-icon-block.flip-horizontal{--outermost--icon-block--transform-scale-x:-1}.wp-block-outermost-icon-block .flip-vertical,.wp-block-outermost-icon-block.flip-vertical{--outermost--icon-block--transform-scale-y:-1}.wp-block-outermost-icon-block .flip-vertical.flip-horizontal,.wp-block-outermost-icon-block.flip-vertical.flip-horizontal{--outermost--icon-block--transform-scale-x:-1;--outermost--icon-block--transform-scale-y:-1}

</style>
<style id='classic-theme-styles-inline-css'>
/*! This file is auto-generated */
.wp-block-button__link{color:#fff;background-color:#32373c;border-radius:9999px;box-shadow:none;text-decoration:none;padding:calc(.667em + 2px) calc(1.333em + 2px);font-size:1.125em}.wp-block-file__button{background:#32373c;color:#fff;text-decoration:none}
</style>
<style id='global-styles-inline-css'>
:root{--wp--preset--aspect-ratio--square: 1;--wp--preset--aspect-ratio--4-3: 4/3;--wp--preset--aspect-ratio--3-4: 3/4;--wp--preset--aspect-ratio--3-2: 3/2;--wp--preset--aspect-ratio--2-3: 2/3;--wp--preset--aspect-ratio--16-9: 16/9;--wp--preset--aspect-ratio--9-16: 9/16;--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--color--brand-color-0: #001b37;--wp--preset--color--brand-color-1: #0a00c7;--wp--preset--color--brand-color-2: #003973;--wp--preset--color--brand-color-3: #fab31e;--wp--preset--color--brand-color-4: #0a3bff;--wp--preset--color--brand-color-5: #deeeff;--wp--preset--color--brand-color-6: #000000;--wp--preset--color--brand-color-7: #ffffff;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 12px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 20px;--wp--preset--font-size--x-large: 42px;--wp--preset--font-size--normal: 18px;--wp--preset--font-size--larger: 24px;--wp--preset--spacing--20: 0.44rem;--wp--preset--spacing--30: 0.67rem;--wp--preset--spacing--40: 1rem;--wp--preset--spacing--50: 1.5rem;--wp--preset--spacing--60: 2.25rem;--wp--preset--spacing--70: 3.38rem;--wp--preset--spacing--80: 5.06rem;--wp--preset--shadow--natural: 6px 6px 9px rgba(0, 0, 0, 0.2);--wp--preset--shadow--deep: 12px 12px 50px rgba(0, 0, 0, 0.4);--wp--preset--shadow--sharp: 6px 6px 0px rgba(0, 0, 0, 0.2);--wp--preset--shadow--outlined: 6px 6px 0px -3px rgba(255, 255, 255, 1), 6px 6px rgba(0, 0, 0, 1);--wp--preset--shadow--crisp: 6px 6px 0px rgba(0, 0, 0, 1);}:where(.is-layout-flex){gap: 0.5em;}:where(.is-layout-grid){gap: 0.5em;}body .is-layout-flex{display: flex;}.is-layout-flex{flex-wrap: wrap;align-items: center;}.is-layout-flex > :is(*, div){margin: 0;}body .is-layout-grid{display: grid;}.is-layout-grid > :is(*, div){margin: 0;}:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-x-large-font-size{font-size: var(--wp--preset--font-size--x-large) !important;}
:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}
:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}
:root :where(.wp-block-pullquote){font-size: 1.5em;line-height: 1.6;}
</style>
<link rel='stylesheet' id='aaai-css' href='https://aaai.org/wp-content/plugins/aaai/public/css/aaai-public.css?ver=1.0.0' media='all' />
<link rel='stylesheet' id='cookie-law-info-controls-css' href='https://aaai.org/wp-content/plugins/cookie-law-info-controls/public/css/cookie-law-info-controls-public.css?ver=1.0.0' media='all' />
<link rel='stylesheet' id='cookie-law-info-css' href='https://aaai.org/wp-content/plugins/cookie-law-info/legacy/public/css/cookie-law-info-public.css?ver=3.2.6' media='all' />
<link rel='stylesheet' id='cookie-law-info-gdpr-css' href='https://aaai.org/wp-content/plugins/cookie-law-info/legacy/public/css/cookie-law-info-gdpr.css?ver=3.2.6' media='all' />
<link rel='stylesheet' id='dashicons-css' href='https://aaai.org/wp-includes/css/dashicons.min.css?ver=6.7.1' media='all' />
<link rel='stylesheet' id='ub-extension-style-css-css' href='https://aaai.org/wp-content/plugins/ultimate-blocks/src/extensions/style.css?ver=6.7.1' media='all' />
<link rel='stylesheet' id='taxopress-frontend-css-css' href='https://aaai.org/wp-content/plugins/simple-tags/assets/frontend/css/frontend.css?ver=3.25.1' media='all' />
<link rel='stylesheet' id='aaai-genesis-child-theme-fonts-css' href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,600,700&#038;display=swap' media='all' />
<link rel='stylesheet' id='shift-styles-css' href='https://aaai.org/wp-content/themes/genesis-sample/shift-styles.css?ver=1.0' media='all' />
<link rel='stylesheet' id='aaai-genesis-child-theme-gutenberg-css' href='https://aaai.org/wp-content/themes/genesis-sample/lib/gutenberg/front-end.css?ver=3.4.1' media='all' />
<style id='aaai-genesis-child-theme-gutenberg-inline-css'>
.gb-block-post-grid .gb-post-grid-items h2 a:hover {
	color: #0073e5;
}

.site-container .wp-block-button .wp-block-button__link {
	background-color: #0073e5;
}

.wp-block-button .wp-block-button__link:not(.has-background),
.wp-block-button .wp-block-button__link:not(.has-background):focus,
.wp-block-button .wp-block-button__link:not(.has-background):hover {
	color: #ffffff;
}

.site-container .wp-block-button.is-style-outline .wp-block-button__link {
	color: #0073e5;
}

.site-container .wp-block-button.is-style-outline .wp-block-button__link:focus,
.site-container .wp-block-button.is-style-outline .wp-block-button__link:hover {
	color: #2396ff;
}		.site-container .has-small-font-size {
			font-size: 12px;
		}		.site-container .has-normal-font-size {
			font-size: 18px;
		}		.site-container .has-large-font-size {
			font-size: 20px;
		}		.site-container .has-larger-font-size {
			font-size: 24px;
		}		.site-container .has-theme-primary-color,
		.site-container .wp-block-button .wp-block-button__link.has-theme-primary-color,
		.site-container .wp-block-button.is-style-outline .wp-block-button__link.has-theme-primary-color {
			color: #0073e5;
		}

		.site-container .has-theme-primary-background-color,
		.site-container .wp-block-button .wp-block-button__link.has-theme-primary-background-color,
		.site-container .wp-block-pullquote.is-style-solid-color.has-theme-primary-background-color {
			background-color: #0073e5;
		}		.site-container .has-theme-secondary-color,
		.site-container .wp-block-button .wp-block-button__link.has-theme-secondary-color,
		.site-container .wp-block-button.is-style-outline .wp-block-button__link.has-theme-secondary-color {
			color: #0073e5;
		}

		.site-container .has-theme-secondary-background-color,
		.site-container .wp-block-button .wp-block-button__link.has-theme-secondary-background-color,
		.site-container .wp-block-pullquote.is-style-solid-color.has-theme-secondary-background-color {
			background-color: #0073e5;
		}
</style>
<link rel='stylesheet' id='simple-social-icons-font-css' href='https://aaai.org/wp-content/plugins/simple-social-icons/css/style.css?ver=3.0.2' media='all' />
<link rel='stylesheet' id='ubermenu-css' href='https://aaai.org/wp-content/plugins/ubermenu/pro/assets/css/ubermenu.min.css?ver=3.7.8' media='all' />
<link rel='stylesheet' id='ubermenu-grey-white-css' href='https://aaai.org/wp-content/plugins/ubermenu/assets/css/skins/blackwhite.css?ver=6.7.1' media='all' />
<link rel='stylesheet' id='ubermenu-font-awesome-all-css' href='https://aaai.org/wp-content/plugins/ubermenu/assets/fontawesome/css/all.min.css?ver=6.7.1' media='all' />
<link rel='stylesheet' id='essential-blocks-frontend-style-css' href='https://aaai.org/wp-content/plugins/essential-blocks/assets/admin/editor/editor.css?ver=e018e5790bb4c227e7ea' media='all' />
<style id='essential-blocks-frontend-style-inline-css'>

            :root {
                --eb-global-primary-color: #101828;
--eb-global-secondary-color: #475467;
--eb-global-tertiary-color: #98A2B3;
--eb-global-text-color: #475467;
--eb-global-heading-color: #1D2939;
--eb-global-link-color: #444CE7;
--eb-global-background-color: #F9FAFB;
--eb-global-button-text-color: #FFFFFF;
--eb-global-button-background-color: #101828;
--eb-gradient-primary-color: linear-gradient(90deg, hsla(259, 84%, 78%, 1) 0%, hsla(206, 67%, 75%, 1) 100%);
--eb-gradient-secondary-color: linear-gradient(90deg, hsla(18, 76%, 85%, 1) 0%, hsla(203, 69%, 84%, 1) 100%);
--eb-gradient-tertiary-color: linear-gradient(90deg, hsla(248, 21%, 15%, 1) 0%, hsla(250, 14%, 61%, 1) 100%);
--eb-gradient-background-color: linear-gradient(90deg, rgb(250, 250, 250) 0%, rgb(233, 233, 233) 49%, rgb(244, 243, 243) 100%);

                --eb-tablet-breakpoint: 1024px;
--eb-mobile-breakpoint: 767px;

            }
            
            
        
</style>
<link rel='stylesheet' id='eb-reusable-block-style-40900-css' href='https://aaai.org/wp-content/uploads/eb-style/reusable-blocks/eb-reusable-40900.min.css?ver=fed3726f07' media='all' />
<script src="https://aaai.org/wp-includes/js/jquery/jquery.min.js?ver=3.7.1" id="jquery-core-js"></script>
<script src="https://aaai.org/wp-includes/js/jquery/jquery-migrate.min.js?ver=3.4.1" id="jquery-migrate-js"></script>
<script id="aaai-js-extra">
var ajax = {"ajaxurl":"https:\/\/aaai.org\/wp-admin\/admin-ajax.php","redirecturl":"https:\/\/aaai.memberclicks.net\/"};
</script>
<script src="https://aaai.org/wp-content/plugins/aaai/public/js/aaai-public.js?ver=1.0.0" id="aaai-js"></script>
<script id="cookie-law-info-js-extra">
var Cli_Data = {"nn_cookie_ids":[],"cookielist":[],"non_necessary_cookies":[],"ccpaEnabled":"","ccpaRegionBased":"","ccpaBarEnabled":"","strictlyEnabled":["necessary","obligatoire"],"ccpaType":"gdpr","js_blocking":"1","custom_integration":"","triggerDomRefresh":"","secure_cookies":""};
var cli_cookiebar_settings = {"animate_speed_hide":"500","animate_speed_show":"500","background":"#FFF","border":"#b1a6a6c2","border_on":"","button_1_button_colour":"#61a229","button_1_button_hover":"#4e8221","button_1_link_colour":"#fff","button_1_as_button":"1","button_1_new_win":"","button_2_button_colour":"#333","button_2_button_hover":"#292929","button_2_link_colour":"#444","button_2_as_button":"","button_2_hidebar":"","button_3_button_colour":"#dedfe0","button_3_button_hover":"#b2b2b3","button_3_link_colour":"#333333","button_3_as_button":"1","button_3_new_win":"","button_4_button_colour":"#dedfe0","button_4_button_hover":"#b2b2b3","button_4_link_colour":"#333333","button_4_as_button":"1","button_7_button_colour":"#61a229","button_7_button_hover":"#4e8221","button_7_link_colour":"#fff","button_7_as_button":"1","button_7_new_win":"","font_family":"inherit","header_fix":"","notify_animate_hide":"1","notify_animate_show":"","notify_div_id":"#cookie-law-info-bar","notify_position_horizontal":"right","notify_position_vertical":"bottom","scroll_close":"","scroll_close_reload":"","accept_close_reload":"","reject_close_reload":"","showagain_tab":"","showagain_background":"#fff","showagain_border":"#000","showagain_div_id":"#cookie-law-info-again","showagain_x_position":"100px","text":"#333333","show_once_yn":"","show_once":"10000","logging_on":"","as_popup":"","popup_overlay":"1","bar_heading_text":"","cookie_bar_as":"banner","popup_showagain_position":"bottom-right","widget_position":"left"};
var log_object = {"ajax_url":"https:\/\/aaai.org\/wp-admin\/admin-ajax.php"};
</script>
<script src="https://aaai.org/wp-content/plugins/cookie-law-info/legacy/public/js/cookie-law-info-public.js?ver=3.2.6" id="cookie-law-info-js"></script>
<script src="https://aaai.org/wp-content/plugins/simple-tags/assets/frontend/js/frontend.js?ver=3.25.1" id="taxopress-frontend-js-js"></script>
<link rel="https://api.w.org/" href="https://aaai.org/wp-json/" /><link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://aaai.org/xmlrpc.php?rsd" />

<script
  src="https://code.jquery.com/jquery-3.6.3.js"
  integrity="sha256-nQLuAZGRRcILA+6dMBOvcRh5Pe310sBpanc6+QBmyVM="
  crossorigin="anonymous"></script>

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-MBH24FG');</script>
<!-- End Google Tag Manager -->
<link rel="stylesheet" href="https://use.typekit.net/fpw2eej.css">
<meta name="google-site-verification" content="4TD1LphPTynHUMWOfM1ONR8Lwa1UTj-mT7Hj_JM1Grw" />
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-GLhlTQ8iRABdZLl6O3oVMWSktQOp6b7In1Zl3/Jr59b6EGGoI1aFkw7cmDA6j6gD" crossorigin="anonymous">
<style id="ubermenu-custom-generated-css">
/** Font Awesome 4 Compatibility **/
.fa{font-style:normal;font-variant:normal;font-weight:normal;font-family:FontAwesome;}

/** UberMenu Custom Menu Styles (Customizer) **/
/* main */
 .ubermenu-main { background-color:#001b37; background:-webkit-gradient(linear,left top,left bottom,from(#001b37),to(#001b37)); background:-webkit-linear-gradient(top,#001b37,#001b37); background:-moz-linear-gradient(top,#001b37,#001b37); background:-ms-linear-gradient(top,#001b37,#001b37); background:-o-linear-gradient(top,#001b37,#001b37); background:linear-gradient(top,#001b37,#001b37); }
 .ubermenu-main .ubermenu-item-level-0 > .ubermenu-target { font-size:18px; color:#f7f7f7; }
 .ubermenu.ubermenu-main .ubermenu-item-level-0:hover > .ubermenu-target, .ubermenu-main .ubermenu-item-level-0.ubermenu-active > .ubermenu-target { color:#ffffff; background:#003973; }
 .ubermenu.ubermenu-main .ubermenu-item-level-0 > .ubermenu-target { background:#001b37; }
 .ubermenu-main .ubermenu-item-level-0.ubermenu-current-menu-item > .ubermenu-target, .ubermenu-main .ubermenu-item-level-0.ubermenu-current-menu-parent > .ubermenu-target, .ubermenu-main .ubermenu-item-level-0.ubermenu-current-menu-ancestor > .ubermenu-target { background:#003973; }
 .ubermenu-main .ubermenu-item.ubermenu-item-level-0 > .ubermenu-highlight { background:#084481; }
 .ubermenu-main .ubermenu-submenu.ubermenu-submenu-drop { background-color:#003973; border:1px solid #003973; color:#f7f7f7; }
 .ubermenu-main .ubermenu-submenu .ubermenu-highlight { color:#ffffff; }
 .ubermenu-main .ubermenu-submenu .ubermenu-item-header > .ubermenu-target { color:#ffffff; }
 .ubermenu-main .ubermenu-item-normal > .ubermenu-target { color:#f7f7f7; font-size:16px; }
 .ubermenu.ubermenu-main .ubermenu-item-normal > .ubermenu-target:hover, .ubermenu.ubermenu-main .ubermenu-item-normal.ubermenu-active > .ubermenu-target { color:#ffffff; }


/** UberMenu Custom Menu Item Styles (Menu Item Settings) **/
/* 230 */    .ubermenu .ubermenu-item-230 > .ubermenu-target.ubermenu-item-layout-image_left > .ubermenu-target-text { padding-left:50px; }
/* 40060 */  .ubermenu .ubermenu-item-40060 > .ubermenu-target.ubermenu-item-layout-image_left > .ubermenu-target-text { padding-left:50px; }
/* 518 */    .ubermenu .ubermenu-item-518 > .ubermenu-target.ubermenu-item-layout-image_left > .ubermenu-target-text { padding-left:50px; }
/* 19835 */  .ubermenu .ubermenu-item-19835 > .ubermenu-target.ubermenu-item-layout-image_left > .ubermenu-target-text { padding-left:50px; }
/* 61189 */  .ubermenu .ubermenu-item-61189 > .ubermenu-target.ubermenu-item-layout-image_left > .ubermenu-target-text { padding-left:50px; }
/* 39821 */  .ubermenu .ubermenu-item-39821 > .ubermenu-target.ubermenu-item-layout-image_left > .ubermenu-target-text { padding-left:50px; }
/* 39822 */  .ubermenu .ubermenu-item-39822 > .ubermenu-target.ubermenu-item-layout-image_left > .ubermenu-target-text { padding-left:50px; }
/* 19897 */  .ubermenu .ubermenu-item-19897 > .ubermenu-target.ubermenu-item-layout-image_left > .ubermenu-target-text { padding-left:50px; }
/* 727 */    .ubermenu .ubermenu-item-727 > .ubermenu-target.ubermenu-item-layout-image_left > .ubermenu-target-text { padding-left:50px; }

/* Status: Loaded from Transient */

</style>
	<style></style>

	<style>  /* Brand Color 0 */
  .has-brand-color-0-color {
    color: #001b37 !important;
  }
  .has-brand-color-0-background-color {
    background-color: #001b37 !important;
  }  /* Brand Color 1 */
  .has-brand-color-1-color {
    color: #0a00c7 !important;
  }
  .has-brand-color-1-background-color {
    background-color: #0a00c7 !important;
  }  /* Brand Color 2 */
  .has-brand-color-2-color {
    color: #003973 !important;
  }
  .has-brand-color-2-background-color {
    background-color: #003973 !important;
  }  /* Brand Color 3 */
  .has-brand-color-3-color {
    color: #fab31e !important;
  }
  .has-brand-color-3-background-color {
    background-color: #fab31e !important;
  }  /* Brand Color 4 */
  .has-brand-color-4-color {
    color: #0a3bff !important;
  }
  .has-brand-color-4-background-color {
    background-color: #0a3bff !important;
  }  /* Brand Color 5 */
  .has-brand-color-5-color {
    color: #deeeff !important;
  }
  .has-brand-color-5-background-color {
    background-color: #deeeff !important;
  }  /* Brand Color 6 */
  .has-brand-color-6-color {
    color: #000000 !important;
  }
  .has-brand-color-6-background-color {
    background-color: #000000 !important;
  }  /* Brand Color 7 */
  .has-brand-color-7-color {
    color: #ffffff !important;
  }
  .has-brand-color-7-background-color {
    background-color: #ffffff !important;
  }</style><link rel="icon" href="https://aaai.org/wp-content/uploads/2022/12/cropped-white-logo-32x32.png" sizes="32x32" />
<link rel="icon" href="https://aaai.org/wp-content/uploads/2022/12/cropped-white-logo-192x192.png" sizes="192x192" />
<link rel="apple-touch-icon" href="https://aaai.org/wp-content/uploads/2022/12/cropped-white-logo-180x180.png" />
<meta name="msapplication-TileImage" content="https://aaai.org/wp-content/uploads/2022/12/cropped-white-logo-270x270.png" />
		<style id="wp-custom-css">
			/* hide meta on conference page */
.single-conference p.entry-meta {
	display: none;
}
.single-conference.no-rule .header-rule {
	border-bottom: none;
}
/* remove rule from h1 on conference page */
h1.no-rule {
	border: none;
}
.date-time {
	text-transform: uppercase;
	font-weight: 700;
	font-family: "Rustica", sans-serif;
	letter-spacing: 1px;
}
@media only screen and (max-width: 600px) {
	.hide-on-mobile {
	display: none;
}
}
.hidden{
	display: none;
}
h2 span.subtitle {
	font-size: 75%;
	display: block;
	padding: 10px 0;
}

h1 span.subtitle {
	display: none;
}

.section-hide {
	display: none;
}

/* Shift Collab edits to tabbed content
 * for custom styling
 * 
 * 
 * * */
:root{
	--primary-blue: #003973;
	--primary-yellow: #fab31e;
	--bright-blue: #0a3bff;
	--light-blue: #deeeff;
	--black: #000;
	--white: #fff;
	--dark-blue-bk: #001b37;
	--bright-blue-bk: #0a00c7;
}

.wp-block-ub-tabbed-content-holder.vertical-holder{
	flex-direction: row-reverse;
}

.wp-block-ub-tabbed-content-tab-title-vertical-wrap.active{
	background-image: linear-gradient(to right, #0a3bff , #003973);
}

.wp-block-ub-tabbed-content-tab-title{
	color: #fff;
}
.wp-block-ub-tabbed-content-tabs-content{
	padding:0px;
}
.wp-block-ub-tabbed-content-tab-content-wrap p{
	margin: 0px;
}
.home .site-container{
	background-color: var(--dark-blue-bk);
}

/* Frontpage styles */
.wp-block-ub-tabbed-content .wp-block-ub-tabbed-content-tabs-content{
	border:none;
}
.ten-border-radius{
	border-radius: 10px;
}
.wp-block-ub-tabbed-content-tab-content-wrap{
	position: relative;
}

.wp-block-ub-tabbed-content-tabs-title-vertical-tab{
	display: grid;
	height: 100%;
}
/* turn long list into columns */
.list-has-columns {
    column-count: 3;
    column-width: 220px;
}



/* Button defaults */
.site-container .wp-block-button .wp-block-button__link{
	border-radius: 35px;
	font-weight: 700 !important;
	border-width: 3px !important;
	border-color: var(--bright-blue);
	padding: 5px 35px !important;
	font-size: 18px !important;
}
/* Calendar quick fixes */
@media(min-width: 700px) {
h2.calendar-heading{
	padding-left: 42px;
}
}
@media(min-width: 900px) {
h2.calendar-heading{
	padding-left: 151px;
}
}
@media(max-width: 700px) {
.calendar-event .event-title,
.calendar-event .event-step,
.calendar-event .display-date,
.calendar-event .event-link {
	width: 100% !important;		
	}
}
.citation-generate{
	display:none;
}
.citation-generate-container.show{
	display:block;
}
.italics{
	font-style:italic;
}
.bold{
	font-weight: bold;
}
/* Text safe defaults */
a{
	color: var(--bright-blue);
	text-decoration: none;
} 
a:hover{
	color: var(--primary-blue);
	text-decoration: underline;
}
.dark-background-color a  {
	color: var(--wp--preset--color--brand-color-3);
}
.dark-background-color a:hover,
.dark-background-color a:focus {
	color: var(--wp--preset--color--cyan-bluish-gray);
}
p {
	color: var(--black);
}
h1,h2,h3,h4,h5{
	color: var(--primary-blue);
		font-family: "Rustica", sans-serif;

}
h1, h1.archive-title {
	font-weight: 500;
	font-size: 45px;
	border-bottom: 1px solid var(--wp--preset--color--brand-color-3);
}
h1.entry-title{
	font-weight: 500;
	font-size: 45px;
	border-bottom:0px;
}
.header-rule{
	width: 100%;
    display: block;
    border-bottom: 1px solid var(--wp--preset--color--brand-color-3);
	margin-bottom: 10px
}
.post-subtitle{
	font-size: 33px;
	color: var(--primary-blue);
  font-family: "Rustica", sans-serif;
	margin-bottom: 10px;
}
.intro-text {
	font-size: 22px;
}
hr {
	border: 1px solid var(--wp--preset--color--brand-color-3);
	
}

.site-header{
	background-image: url(https://aaaiprod.wpengine.com/wp-content/uploads/2022/12/header-texture.png);
	background-repeat: no-repeat;
	background-position: center;
	background-size: cover;
}

#close-icon-mobile{
	display:none;
	float:right;
	color:#fff;
	margin-right:30px;
	font-size: 50px;
	height:0;
}
#close-icon{
	color:#fff;
	cursor: pointer;
}
.title-area{
		display:none;
	}

/* Fix for in-page anchors, so the anchor text is not under the header */
.in-page-anchor:before {
    content: '';
    display: block;
    position: relative;
    width: 0;
    height: 4em;
    margin-top: -4em;
	z-index: -10 !important;
}
.in-page-anchor {
	z-index: -10 !important;
	
}


/* temp fix for conference and paper dates */
.conference .entry-header .entry-meta,
.papers .entry-header .entry-meta {
	display: none;
}

@media(max-width: 899px){
	/* Mobile menu styles */
.ubermenu .ubermenu-submenu .ubermenu-target-text {
		width: 100%;
		margin-left: 80px;
	}
	.ubermenu .ubermenu-submenu .ubermenu-submenu .ubermenu-target-text{
		margin-left: 120px;
	}
	.side-menu{
    background-color: var(--dark-blue-bk) !important;
		z-index: 999999 !important;
		padding-top: 5px !important;
}
	
	.wp-block-ub-tabbed-content-tabs-title-vertical-tab > div:not(:last-child){
		border-right:0;
		border-top-left-radius: 0px;
    border-top-right-radius: 0px;
    border-bottom-left-radius: 0px;
    border-bottom-right-radius: 0px;
	}
	
	.wp-block-genesis-blocks-gb-column .gb-block-layout-column-inner .wp-block-button > a{
		width: 100%;
    max-width: 400px;
	}
	.wp-block-button{
		width: 100%;
    max-width: 400px;
	}
.gb-layout-column-wrap{
    grid-template-areas: none;
    grid-template-columns: none;
    grid-gap: 0 1em;
	}
	.wp-container-1{
		flex-direction: column;
		margin-bottom: 15px;
	}
	
	.wp-block-ub-tabbed-content-tabs-title-vertical-tab div:last-child{
		border-top-left-radius: 0px;
    border-top-right-radius: 0px;
    border-bottom-left-radius: 10px;
    border-bottom-right-radius: 10px;
		border-left: 0;
    border-right: 0;
    border-bottom: 0;
	}

	
.wp-block-ub-tabbed-content-holder.vertical-holder {
    display: flex;
    flex-flow: column;
    flex-direction: column-reverse;
}
	
.wp-block-ub-tabbed-content-tab-content-wrap:after {
    content: '';
    position: absolute;
    bottom: 0;
    left: 0;
    right: 0;
    height: 300px;
    background: transparent;
}

.wp-block-ub-tabbed-content-tab-content-wrap:before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    height: 300px;
    z-index: 1;
    background: linear-gradient(0deg, rgba(0,0,0,0), rgba(0,0,0,0.9));
}
	
	.wp-block-ub-tabbed-content-tabs-title-mobile-horizontal-tab{
		display:block;
	}
			.wp-block-ub-tabbed-content-tabs-content, .wp-block-ub-tabbed-content-tab-content-wrap div{
				padding-top:0.5em !important;
				padding-bottom:0em !important;
				
				-webkit-border-top-left-radius: 0px;
			-webkit-border-bottom-right-radius: 0px;
		-webkit-border-bottom-left-radius: 0px;
				-moz-border-radius-topleft: 0px;
		-moz-border-radius-bottomright: 0px;
		-moz-border-radius-bottomleft: 0px;
				border-top-left-radius: 0px;
		border-bottom-right-radius: 0px;
		border-bottom-left-radius: 0px;
		}
}

/* sidebar nav stylings */
.ubermenu-submenu{
    display: flex;
    flex-flow: row wrap;
    align-items: center;
		min-width:220px
}
.ubermenu-skin-grey-white .ubermenu-submenu.ubermenu-submenu-drop{
	background-color: var(--primary-blue);
}
.ubermenu-wrap .ubermenu-submenu > .ubermenu-item{
	color: #fff;
	background-color: var(--primary-blue);
}
.ubermenu-wrap .ubermenu-submenu {
	border-right: 2px solid #084481 !important;
}

.ubermenu-wrap .ubermenu-submenu .ubermenu-submenu,.ubermenu-wrap .ubermenu-submenu .ubermenu-submenu > .ubermenu-item{
	min-width:220px;
	background-color: #084481;
}
.ubermenu-submenu.ubermenu-submenu-type-flyout{
	min-width: 220px;
}
.ubermenu .ubermenu-target{
    display:flex;
}
.ubermenu .ubermenu-target-with-image>.ubermenu-target-text{
    display: flex;
    align-items: center;
}
.ubermenu .ubermenu-submenu .ubermenu-target-text{
	width: 100%;
  padding: 5px 10px;
}
.ubermenu .ubermenu-submenu .ubermenu-target-text:hover, 
.ubermenu .ubermenu-submenu .ubermenu-current-menu-item .ubermenu-target-text {
	background-color: var(--bright-blue);
	border-radius: 5px;
}

.ubermenu .ubermenu-submenu .ubermenu-current-menu-item .ubermenu-submenu .ubermenu-target-text {
	background-color: #084481;
}
.ubermenu .ubermenu-submenu .ubermenu-current-menu-item .ubermenu-submenu .ubermenu-target-text:hover, 
.ubermenu .ubermenu-submenu .ubermenu-current-menu-item .ubermenu-submenu .ubermenu-target-text:focus {
	background-color: var(--bright-blue);	
}

.ubermenu .ubermenu-submenu .ubermenu-current-menu-item > .ubermenu-target {
	color: white;
}
.ubermenu .ubermenu-submenu .ubermenu-target {
	padding: 5px 20px;
}
.author-wrap{
	margin-bottom: 1em;
	display: flex;
}
.author-wrap h4{
	margin: 0;
  justify-self: flex-start;
  flex: 20%; 
}
.author-wrap > div{
	flex: 80%;
	justify-self: flex-start;
  text-align: left;
}
.author-wrap p{
	margin: 0px 0px 2px;
	padding-left: 8em;
}

.paper-section-wrap{
	margin-bottom: 1em;
	display: flex;
}
.paper-section-wrap h4{
	margin: 0;
  justify-self: flex-start;
  flex: 20%; 
}
.paper-section-wrap > div{
	flex: 80%;
	justify-self: flex-start;
  text-align: left;
}
.paper-section-wrap p{
	margin: 0px 0px 2px;
	padding-left: 8em;
}

.paper-section-wrap .pdf-button{
	margin: 0px 0px 2px;
	padding-left: 8em;
}
.bold{
	font-weight: bold;
}
/*.entry-content p:first-child{
	background-color: #DEEEFF;
	border-radius: 10px;
	padding: 20px 15px;
	font-weight: bold;
}
*/
.breadcrumbs{
	margin: 0px 0px 50px;
  padding: 2px 4px;
	text-transform: uppercase !important;
}
.breadcrumbs > a{
	font-size: 16px;
	color: #000;
	text-decoration:none;
}
.breadcrumb {
	font-family: "Roboto", sans-serif;
	font-weight: 400;
	font-size: 16px;
	border-bottom: none;
	margin-top: 0;
	padding-top: 0;
}
.breadcrumb a {
	color: var(--black);
}
.papers-sidebar{
	font-size: 15px;
}
.track-wrap h5{
	margin:0px
}
.papers-author-page{
	display: flex;
  flex-direction: row;
 	justify-content: space-between;
}
.papers-author-page p{
	margin: 0px
}
.papers-author-page p:first-child{
	width: 70%;
}
.paper-wrap{
	margin: 0px 0px 15px;
}
.paper-wrap .wp-block-button{
	font-weight: bold;
	text-decoration:underline;
}
.pdf-button{
	padding: 2em 0em;
  display: inline-block;
}
.pdf-button a{
	border-radius: 35px;
	padding: 5px 20px;
  background-color: transparent;
  border: 2px solid currentColor;
}
#search-form .input-group{
		display: flex;
		align-items: center;
    justify-content: center;
}
#search-form input{
		width: 80%;
		margin-left:10px;
    background: transparent;
    border-width: 0px 0px 2px;
}
#search-input, #search-input::placeholder{
	color: #fff;
}
.fa-search::before{
	color:#fff;
	
}
.text-bubble:before{
	content:url("https://aaaiprod.wpengine.com/wp-content/uploads/2022/12/chat-1.png");
}
.open-book:before{
	content:url("https://aaaiprod.wpengine.com/wp-content/uploads/2022/12/book-open-1.png");
}
.creased-paper:before{
	content:url("https://aaaiprod.wpengine.com/wp-content/uploads/2022/12/file-filled-1.png");
}
.portrait:before{
	content:url("https://aaaiprod.wpengine.com/wp-content/uploads/2022/12/user-square-1.png");
}

.subtitle{
	display:none;
	font-size: 22px;
  padding: 0 0 10px;
}
.blog .subtitle{
	display: block;
}
.dashicons{
	cursor: pointer;
}

.side-menu-header{
	  position: fixed;
    top: 20px;
}
.site-inner{
	min-height:1200px;
	padding-top: 20px;
}

/* Papers styles */
.blue-box{
	background: #DEEEFF;
	border-radius: 10px;
	padding: 20px 15px;
	display: flex;
}
.blue-box img{
	width: auto;
	max-height: 250px;
	float: left;
	padding-right: 20px;
}
.blue-box .blue-box-attributes p{
	margin: 0px;
}
.doi-output p{
	margin: 0px 0px 2px;
  padding-left: 6em;
}
.abstract-output p{
	margin: 0px 0px 2px;
  padding-left: 6em;
}
/* sidemenu styles */
.ubermenu-wrap{
	background-color: var(--dark-blue-bk);
	z-index:9;
	position: sticky;
  padding-top: 0px;
	top: 0px;
}
.menu-control-wrap{
    display:none;
}
.ubermenu .ubermenu-divider hr{
	margin: 0 auto;
  width: 200px;
  background-color: var(--bright-blue);
}
.conf-wrap .site-inner .content-sidebar-wrap .sidebar{
	margin-top: 130px;
		float:left;
	}

.conf-sidebar-wrapper ul {
    padding-left: 0;
    list-style: none;
}

.conf-sidebar-wrapper ul li {
    margin-left: 0;
}

.conf-sidebar-wrapper h4,
.conf-sidebar-wrapper h5 {
	margin-top: 2.5em;
}

@media(min-width:960px){

	.ubermenu-wrap{
		position: fixed;
		height: 100%;
		max-width: 260px;
		top: unset;
	}
	.menu-control-wrap{
		display:block;
	}
	.ubermenu-submenu-type-flyout{
		height:2500px!important;
	}
	.ubermenu-sub-indicator {
    display: none;
	}

	.archive .breadcrumb, 
	.blog .breadcrumb, 
	.single .breadcrumb {
		margin-left: 0;
	}

	.archive .entry-header,
	.blog .entry-header ,
	.single .entry-header {
		margin-left: 0px;
	}
}
.ubermenu .ubermenu-submenu .ubermenu-column-auto{
	min-width: auto;
}

.proceeding-breadcrumb{
	display:flex;
	gap: 10px;
	margin-bottom: 2em;
}

.breadcrumb{
	display:flex;
	gap: 10px;
	margin-bottom: 2em;
}

@media (min-width:960px) and (max-width:1584px){
	.home .site-container .site-inner{
		margin-right: 20px;
    width: calc(100% - 330px);
	}
}


/* evergreen conf styles */
.conference-header-wrap {
  display: flex;
  flex-wrap: wrap;
  justify-content: space-between;
  align-items: center;
	background-color: #003973;
}
.spacer{
	flex-basis:10%;
}
.conference-header-text {
  flex-basis: 50%;
	padding: 0em 1em;
}
.conference-header-text h3{
	color:#fff;
	font-style: normal;
	font-weight: 500;
	font-size: 40px;
	line-height: 45px;
}
.conference-header-text h4{
	color:#fff;
	font-style: normal;
	font-weight: 700;
	font-size: 18px;
	line-height: 30px;
	letter-spacing: 0.05em;
	text-transform: uppercase;
}
.conference-header-image {
  flex-basis: 35%;
	position: relative;
	width: 100%;
}
.conference-header-image::before {
  content: "";
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background: linear-gradient(to left, rgba(0, 57, 115, 0), #003973);
  z-index: 1;
}
.conference-header-image::after {
  content: "";
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background: linear-gradient(to bottom, rgba(0, 57, 115, 0.0), #003973);
  z-index: 1;
	display:none;
}
.conference-header-image img {
  width: 100%;
  height: auto;
}

.current-conference-wrap .content{
	width: 100%;
}

.conf-sidebar-wrapper ul > li{
	list-style-type: none;
}
.conference-header-text {
    display: flex;
    align-items: center;
}
.conference-header-logo {
    margin-right: 20px;
	align-self: start;
}

h1.archive-title{
	font-size: 30px;
}
/* current conf page menu styles */
/* Desktop styles */
@media (min-width: 943px) {
    /* Hide mobile menu */
    #current-conference-menu {
        display: none;
    }
    /* Show desktop menu */
    #current-conference-menu.desktop-menu {
        display: block !important;
    }
    #current-conference-menu.desktop-menu .nav-item {
        display: inline-block;
        margin-right: 10px;
    }
}
.navbar ul > li{
	list-style-type: none;
}
.hidden-text > a > span{
	display:none !important;
}
.hidden-text > a > i:before{
	font-size: 2em;
}
/* Mobile styles */
@media (max-width: 942px) {
	/* Hide mobile menu by default */
    #current-conference-menu {
        display: none;
    }
    /* Hide desktop menu */
    #current-conference-menu.desktop-menu {
        display: none !important;
    }
    /* Show mobile menu */
    #current-conference-menu {
        display: block;
    }
    #current-conference-menu .nav-item {
        display: block;
        margin-bottom: 10px;
    }
	.conference-header-wrap{
		flex-flow: column-reverse;
	}
	.conference-header-image::before{
		display:none
	}
	.conference-header-image::after{
		display:block;
	}
	.spacer{
	flex-basis:0%;
}
}

.navbar-toggler{
	   display: flex;
    justify-content: space-between;
    align-items: center;
    width: 100%;
    border: none;
    border-bottom: 2px solid blue;
    border-radius: 0px;
}
.fa-caret-down {
  transform: rotate(180deg);
}

@media (max-width: 768px) {
  .conference-header-text,
  .conference-header-image {
    flex-basis: 100%;
  }
}

.ubermenu-item a{
	padding: 5px 20px;
}
#search-form{
	margin: 0px!important;
}
.ubermenu-target {
	padding: 5px 20px!important;
}

.navbar-nav{
	padding-left: 0px!important;
	border-bottom: 2px solid #DEEEFF;
}
.navbar-nav a{
	color: #0A3BFF;
	font-weight:bold;
	font-family: "Roboto", sans-serif;
}
.nav-item:hover{
	border-bottom: 2px solid blue;
}
.navbar-nav a:hover{
	color: #0A3BFF;
	text-decoration: none;
	padding-bottom: 3px;
}
.navbar-expand-lg .navbar-nav{
	  width: 100%;
    gap: 4%;
}

/* conference menu */
.menu-current-conference-midpage-container, 
.menu-conference-aaai-25-container, 
.menu-conference-aaai-26-container{
	padding-top: 10px
}
@media only screen and (max-width: 959px) {
	.menu-current-conference-midpage-container, 
.menu-conference-aaai-25-container, 
.menu-conference-aaai-26-container{
		width: 100%
	}
  .menu-current-conference-midpage-container ul, 
.menu-conference-aaai-25-container ul, 
.menu-conference-aaai-26-container ul {
    flex-wrap: wrap;
  }

  .menu-current-conference-midpage-container li, 
.menu-conference-aaai-25-container li, 
.menu-conference-aaai-26-container li {
    flex-basis: 10%;
  }
	
	.author-wrap{
		flex-direction: column;
	}
	.author-wrap > div p{
		padding-left: 0px;
	}
	.paper-section-wrap{
		flex-direction: column;
	}
	.paper-section-wrap .pdf-button{
		padding-left: 0px;
	}
	.paper-section-wrap > div p{
		padding-left: 0px;
	}
	.blue-box{
		flex-direction: column;
	}
	.blue-box img{
    max-height: 100%;
	}
}
@media only screen and (min-width: 960px){
	.menu-current-conference-midpage-container, 
.menu-conference-aaai-25-container, 
.menu-conference-aaai-26-container {
		margin-left: -180px;
		margin-right: -180px;
		max-width: calc(100% + 360px);
		width: auto;
	}
}
.menu-current-conference-midpage-container ul, 
.menu-conference-aaai-25-container ul, 
.menu-conference-aaai-26-container ul {
  list-style: none;
  padding: 0;
  margin: 0;
  display: flex;
}

.menu-current-conference-midpage-container ul > li, 
.menu-conference-aaai-25-container ul > li, 
.menu-conference-aaai-26-container ul > li {
  margin-right: 10px;
  min-width: 120px;
	list-style: none;
	text-align: center;
}

.menu-current-conference-midpage-container li:last-child, 
.menu-conference-aaai-25-container li:last-child, 
.menu-conference-aaai-26-container li:last-child {
  margin-right: 0;
}

.menu-current-conference-midpage-container a, 
.menu-conference-aaai-25-container a, 
.menu-conference-aaai-26-container a {
  text-decoration: none;
  display: block;
  padding: 5px 10px;
}

.menu-current-conference-midpage-container a:hover, 
.menu-conference-aaai-25-container a:hover, 
.menu-conference-aaai-26-container a:hover {
  text-decoration: underline;
}



@media (min-width: 943px) {
	.navbar-expand-lg .navbar-nav a:first-of-type{
		padding-left:0px!important;
	}
}
@media (max-width: 943px) {
	.nav-item:hover{
		border-bottom: 0px solid blue;
	}
	.navbar-nav a:hover{
		padding-bottom: 8px;
	}
	.navbar-toggler:hover{
		background-color: #fff;
    border-width: 2px;
    color: grey;
	}
	.navbar-toggler:focus{
		outline: 0px!important;
		box-shadow: none;
	}
}

/* events display */
.event-header-block{
	padding: 10px 10px 0px;
}
.event-date-block > *:nth-child(odd){
	background-color: #E5E5E5;
}
.event-date-block > *:nth-child(even) {
  background-color: #FFFFFF;
}
.program-event-block{
	display:flex;
	align-items: start;
  gap: 1rem; 
  padding: 1rem; 
}
.program-time-block{
	min-width: 150px;
	font-weight: bold;
}
.program-event-block > *:first-child{
	margin-left: 20px;
}
.program-event-block > *:last-child{
	margin-left: 20px;
	flex-basis: 85%;
}
.event-date-block > *:last-child{
  margin-bottom: 20px;
}
@media (max-width: 993px){
	.program-event-block{
		display: block;
		margin-left: 20px
	}
	.program-time-block{
		padding: 20px;
	}
	.program-event-block > *:first-child{
		margin-left: 0px;
		
	}
}

/* Hide the submenu by default */
#menu-current-conference-midpage .sub-menu, 
.menu-conference-aaai-25-container .sub-menu, 
.menu-conference-aaai-26-container .sub-menu {
  height: 0;
  overflow: hidden;
  transition: max-height 0.3s ease;
  position: absolute;
  z-index: 9999;
  background-color: white;
}

#menu-current-conference-midpage li:hover > .sub-menu, 
.menu-conference-aaai-25-container li:hover > .sub-menu, 
.menu-conference-aaai-26-container li:hover > .sub-menu {
  height: auto;
	border: 1px solid blue;
}

#menu-current-conference-midpage .sub-menu li, 
.menu-conference-aaai-25-container .sub-menu li, 
.menu-conference-aaai-26-container .sub-menu li {
  padding: 5px 0;
  list-style: none;
  display: block;
	width: 100%;
}

#menu-current-conference-midpage .sub-menu a, 
.menu-conference-aaai-25-container .sub-menu a, 
.menu-conference-aaai-26-container .sub-menu a {
  color: #000000;
  text-decoration: none;
	text-align: left;
}

#menu-current-conference-midpage .sub-menu, 
.menu-conference-aaai-25-container .sub-menu, 
.menu-conference-aaai-26-container .sub-menu {
  display: block;
  opacity: 0;
}

#menu-current-conference-midpage li:hover > .sub-menu, 
.menu-conference-aaai-25-container li:hover > .sub-menu, 
.menu-conference-aaai-26-container li:hover > .sub-menu {
  opacity: 1;
}
#menu-current-conference-midpage .sub-menu li:hover, 
.menu-conference-aaai-25-container .sub-menu li:hover, 
.menu-conference-aaai-26-container .sub-menu li:hover{
	background-color: #FAFAFA;
}

/* part of the conference template fix 11/22/2023 */
.single-conference .content-sidebar-wrap{
	display: flex;
}
.single-conference .content-sidebar-wrap aside{
	order: -1;
}
.single-conference .content-sidebar-wrap .conf-sidebar-wrapper{
	width: 250px;
	margin-right: 20px;
}

/* 11/22/2023 future conference fix stylings */
.conf-wrap.future-conf .breadcrumb{
	display: none;
}
.conf-wrap.future-conf .entry-meta{
	display: none;
}
.conf-wrap.future-conf .header-rule{
	display: none;
}
.conf-wrap.future-conf .site-inner{
	width: 100%;
	margin-right: auto;
}

.full-width-content .entry-content > .alignwide{
	margin-left: 0px;
}
.menu-current-conference-midpage-container, .menu-conference-aaai-25-container, .menu-conference-aaai-26-container{
	margin-left: 0px;
}

/** conf template update 11/26/2024 for stacking the sidebar **/
@media (max-width: 943px) {
	.single-conference .content-sidebar-wrap{
		flex-direction: column;
	}
	.single-conference .content-sidebar-wrap aside {
    order: 1;
	}
}

@media (min-width: 960px) and (max-width: 1584px) {
    .conf-wrap .site-inner {
        margin-right: auto !important;
        width: calc(100% - 330px);
    }
}

/** gravity forms **/
.gform_wrapper.gravity-theme .gsection {
	border-bottom: none;
	border-top: 1px solid var(--wp--preset--color--brand-color-3);
	padding-top: 1em;
}		</style>
		</head>
<body class="archive tax-proceeding term-vol-34-no-07-aaai-20-technical-tracks-7 term-4560 wp-custom-logo wp-embed-responsive header-full-width content-sidebar genesis-breadcrumbs-hidden genesis-footer-widgets-hidden no-js">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MBH24FG"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
	<script>
	//<![CDATA[
	(function(){
		var c = document.body.classList;
		c.remove( 'no-js' );
		c.add( 'js' );
	})();
	//]]>
	</script>
	<div class="proceedings-wrap"><ul class="genesis-skip-link"><li><a href="#genesis-content" class="screen-reader-shortcut"> Skip to main content</a></li><li><a href="#genesis-sidebar-primary" class="screen-reader-shortcut"> Skip to primary sidebar</a></li></ul><header class="site-header"><div class="wrap"><div class="title-area"><a href="https://aaai.org/" class="custom-logo-link" rel="home"><img width="1000" height="235" src="https://aaai.org/wp-content/uploads/2024/03/AAAI-Logo-Title-White.png" class="custom-logo" alt="AAAI" decoding="async" fetchpriority="high" srcset="https://aaai.org/wp-content/uploads/2024/03/AAAI-Logo-Title-White.png 1000w, https://aaai.org/wp-content/uploads/2024/03/AAAI-Logo-Title-White-300x71.png 300w, https://aaai.org/wp-content/uploads/2024/03/AAAI-Logo-Title-White-768x180.png 768w" sizes="(max-width: 1000px) 100vw, 1000px" /></a><p class="site-title">AAAI</p><p class="site-description">Association for the Advancement of Artificial Intelligence</p></div>	<div class="menu-control-wrap">
		
<!-- UberMenu [Configuration:main] [Theme Loc:] [Integration:api] -->
<button class="ubermenu-responsive-toggle ubermenu-responsive-toggle-main ubermenu-skin-grey-white ubermenu-loc- ubermenu-responsive-toggle-content-align-left ubermenu-responsive-toggle-align-full " tabindex="0" data-ubermenu-target="ubermenu-main-44"><i class="fas fa-bars" ></i>Menu</button><nav id="ubermenu-main-44" class="ubermenu ubermenu-nojs ubermenu-main ubermenu-menu-44 ubermenu-responsive ubermenu-responsive-default ubermenu-mobile-accordion ubermenu-responsive-collapse ubermenu-vertical ubermenu-transition-none ubermenu-trigger-hover_intent ubermenu-skin-grey-white  ubermenu-bar-align-left ubermenu-items-align-flex ubermenu-bound ubermenu-disable-submenu-scroll ubermenu-sub-indicators ubermenu-retractors-responsive ubermenu-submenu-indicator-closes"><ul id="ubermenu-nav-main-44" class="ubermenu-nav" data-title="Menu Controls"><!-- begin Tabs: [Tabs] 561 --><li id="menu-item-561" class="ubermenu-item ubermenu-tabs ubermenu-item-561 ubermenu-item-level-0 ubermenu-column ubermenu-column-full ubermenu-tab-layout-top ubermenu-tabs-show-default ubermenu-tabs-show-current"><ul  class="ubermenu-tabs-group ubermenu-tabs-group--trigger-mouseover ubermenu-column ubermenu-column-full ubermenu-submenu ubermenu-submenu-id-561 ubermenu-submenu-type-auto ubermenu-submenu-type-tabs-group"  ><li id="menu-item-562" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-ubermenu-custom ubermenu-item-562 ubermenu-item-auto ubermenu-item-level-1 ubermenu-column ubermenu-column-auto" ><div class="ubermenu-content-block ubermenu-custom-content ubermenu-custom-content-padded"><i id="close-icon" class="fas fa-bars" style="font-size: 25px;"></i></div></li><li id="menu-item-563" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-ubermenu-custom ubermenu-item-563 ubermenu-item-auto ubermenu-item-level-1 ubermenu-column ubermenu-column-auto" ><div class="ubermenu-content-block ubermenu-custom-content ubermenu-custom-content-padded"><div class="custom-title-area">
<a href="https://aaaiprod.wpengine.com/" class="custom-logo-link" rel="home" aria-current="page">
<img width="284" height="43" src="https://aaai.org/wp-content/uploads/2024/03/AAAI-Logo-Title-White.png" class="custom-logo" alt="AAAI">
</a>
<p class="site-title">AAAI</p>
<p class="site-description">Association for the Advancement of Artificial Intelligence</p>
</div></div></li></ul></li><!-- end Tabs: [Tabs] 561 --></ul></nav>
<!-- End UberMenu -->
	</div>
</div></header>	<div class="ubermenu-wrap">
		
<!-- UberMenu [Configuration:main] [Theme Loc:] [Integration:api] -->
<button class="ubermenu-responsive-toggle ubermenu-responsive-toggle-main ubermenu-skin-grey-white ubermenu-loc- ubermenu-responsive-toggle-content-align-left ubermenu-responsive-toggle-align-full " tabindex="0" data-ubermenu-target="ubermenu-main-21"><i class="fas fa-bars" ></i>Menu</button><nav id="ubermenu-main-21" class="ubermenu ubermenu-nojs ubermenu-main ubermenu-menu-21 ubermenu-responsive ubermenu-responsive-default ubermenu-mobile-accordion ubermenu-responsive-collapse ubermenu-vertical ubermenu-transition-none ubermenu-trigger-hover_intent ubermenu-skin-grey-white  ubermenu-bar-align-left ubermenu-items-align-flex ubermenu-bound ubermenu-disable-submenu-scroll ubermenu-sub-indicators ubermenu-retractors-responsive ubermenu-submenu-indicator-closes"><ul id="ubermenu-nav-main-21" class="ubermenu-nav" data-title="upper-nav"><li id="menu-item-61136" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-ubermenu-custom ubermenu-item-61136 ubermenu-item-level-0 ubermenu-column ubermenu-column-auto" ><div class="ubermenu-content-block ubermenu-custom-content ubermenu-custom-content-padded"><form role="search" method="get" id="search-form" action="https://aaai.org/" class="input-group mb-3">
  <div class="input-group">
    <input type="search" class="form-control border-0" placeholder="Search" aria-label="search nico" name="s" id="search-input" value="">
      <div class="input-group-append">
         <span class="input-group-append p-0">
          <i class="fas fa-search text-muted"></i>
         </span>
    </div>
  </div>
</form></div></li><li id="menu-item-19835" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-has-children ubermenu-item-19835 ubermenu-item-level-0 ubermenu-column ubermenu-column-auto ubermenu-has-submenu-drop ubermenu-has-submenu-flyout" ><a class="ubermenu-target ubermenu-target-with-image ubermenu-item-layout-default ubermenu-item-layout-image_left" href="https://aaai.org/about-aaai/" tabindex="0"><img class="ubermenu-image ubermenu-image-size-full" src="https://aaai.org/wp-content/uploads/2023/03/aaai-icon_about-line-yellow.png" width="40" height="40" alt="About AAAI"  /><span class="ubermenu-target-title ubermenu-target-text">About AAAI</span><i class='ubermenu-sub-indicator fas fa-angle-down'></i></a><ul  class="ubermenu-submenu ubermenu-submenu-id-19835 ubermenu-submenu-type-flyout ubermenu-submenu-drop ubermenu-submenu-align-left_edge_item"  ><li id="menu-item-39857" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-current_page_parent ubermenu-item-39857 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/news/"><span class="ubermenu-target-title ubermenu-target-text">News</span></a></li><li id="menu-item-40109" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-40109 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-officers-and-committees/"><span class="ubermenu-target-title ubermenu-target-text">Officers and Committees</span></a></li><li id="menu-item-61147" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-61147 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-staff/"><span class="ubermenu-target-title ubermenu-target-text">Staff</span></a></li><li id="menu-item-40941" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-40941 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/bylaws-of-aaai/"><span class="ubermenu-target-title ubermenu-target-text">Bylaws</span></a></li><li id="menu-item-724" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-has-children ubermenu-item-724 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1 ubermenu-has-submenu-drop ubermenu-has-submenu-flyout ubermenu-flyout-full-height" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-awards/"><span class="ubermenu-target-title ubermenu-target-text">Awards</span><i class='ubermenu-sub-indicator fas fa-angle-down'></i></a><ul  class="ubermenu-submenu ubermenu-submenu-id-724 ubermenu-submenu-type-flyout ubermenu-submenu-drop ubermenu-submenu-align-vertical_full_height"  ><li id="menu-item-19883" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-19883 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-awards/the-aaai-fellows-program/"><span class="ubermenu-target-title ubermenu-target-text">Fellows Program</span></a></li><li id="menu-item-19884" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-19884 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-awards/aaai-classic-paper-award/"><span class="ubermenu-target-title ubermenu-target-text">Classic Paper Award</span></a></li><li id="menu-item-19885" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-19885 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-awards/aaai-acm-sigai-doctoral-dissertation-award/"><span class="ubermenu-target-title ubermenu-target-text">Dissertation Award</span></a></li><li id="menu-item-19886" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-19886 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-awards/aaai-distinguished-service-award/"><span class="ubermenu-target-title ubermenu-target-text">Distinguished Service Award</span></a></li><li id="menu-item-19887" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-19887 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" target="_blank" href="http://awards.acm.org/newell/"><span class="ubermenu-target-title ubermenu-target-text">Allen Newell Award</span></a></li><li id="menu-item-19888" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-19888 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-awards/aaai-conference-paper-awards-and-recognition/"><span class="ubermenu-target-title ubermenu-target-text">Outstanding Paper Award</span></a></li><li id="menu-item-19889" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-19889 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-awards/aaai-award-for-artificial-intelligence-for-the-benefit-of-humanity/"><span class="ubermenu-target-title ubermenu-target-text">Award for Artificial Intelligence for the Benefit of Humanity</span></a></li><li id="menu-item-19890" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-19890 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-awards/aaai-feigenbaum-prize/"><span class="ubermenu-target-title ubermenu-target-text">Feigenbaum Prize</span></a></li><li id="menu-item-19891" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-19891 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-awards/aaai-eaai-patrick-henry-winston-outstanding-educator-award/"><span class="ubermenu-target-title ubermenu-target-text">Patrick Henry Winston Outstanding Educator Award</span></a></li><li id="menu-item-19892" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-19892 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-awards/robert-s-engelmore-memorial-lecture-award/"><span class="ubermenu-target-title ubermenu-target-text">Engelmore Award</span></a></li><li id="menu-item-19893" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-19893 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-awards/aaai-isef-awards/"><span class="ubermenu-target-title ubermenu-target-text">AAAI ISEF Awards</span></a></li><li id="menu-item-19894" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-19894 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-awards/aaai-senior-member-status/"><span class="ubermenu-target-title ubermenu-target-text">Senior Member Status</span></a></li><li id="menu-item-19895" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-19895 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-awards/aaai-conference-awards/"><span class="ubermenu-target-title ubermenu-target-text">Conference Awards</span></a></li></ul></li><li id="menu-item-90435" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-90435 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/partnerships/"><span class="ubermenu-target-title ubermenu-target-text">Partnerships</span></a></li><li id="menu-item-40939" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-40939 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-resources/"><span class="ubermenu-target-title ubermenu-target-text">Resources</span></a></li><li id="menu-item-61198" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-61198 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/mailing-lists/"><span class="ubermenu-target-title ubermenu-target-text">Mailing Lists</span></a></li><li id="menu-item-744" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-744 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/past-aaai-presidential-addresses/"><span class="ubermenu-target-title ubermenu-target-text">Past Presidential Addresses</span></a></li><li id="menu-item-72785" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-72785 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-presidential-panel-on-long-term-ai-futures-2008-2009/"><span class="ubermenu-target-title ubermenu-target-text">Presidential Panel on Long-Term AI Futures</span></a></li><li id="menu-item-750" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-has-children ubermenu-item-750 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1 ubermenu-has-submenu-drop ubermenu-has-submenu-flyout ubermenu-flyout-full-height" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/ai-science-policy/"><span class="ubermenu-target-title ubermenu-target-text">Past Policy Reports</span><i class='ubermenu-sub-indicator fas fa-angle-down'></i></a><ul  class="ubermenu-submenu ubermenu-submenu-id-750 ubermenu-submenu-type-flyout ubermenu-submenu-drop ubermenu-submenu-align-vertical_full_height"  ><li id="menu-item-19840" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-19840 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/ai-science-policy/the-role-of-intelligent-systems-in-the-national-information-infrastructure/"><span class="ubermenu-target-title ubermenu-target-text">The Role of Intelligent Systems in the National Information Infrastructure (1995)</span></a></li><li id="menu-item-749" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-749 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-2" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/ai-science-policy/a-report-to-arpa-on-twenty-first-century-intelligent-systems/"><span class="ubermenu-target-title ubermenu-target-text">A Report to ARPA on Twenty-First Century Intelligent Systems (1994)</span></a></li></ul></li><li id="menu-item-40945" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-40945 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/about-aaai/aaai-logos/"><span class="ubermenu-target-title ubermenu-target-text">Logos</span></a></li></ul></li><li id="menu-item-39822" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-39822 ubermenu-item-level-0 ubermenu-column ubermenu-column-auto ubermenu-align-left" ><a class="ubermenu-target ubermenu-target-with-image ubermenu-item-layout-default ubermenu-item-layout-image_left" href="https://aaai.org/about-aaai/ethics-and-diversity/" tabindex="0"><img class="ubermenu-image ubermenu-image-size-full" src="https://aaai.org/wp-content/uploads/2023/03/aaai-icon_ethics-diversity-line-yellow.png" width="40" height="40" alt="aaai-icon_ethics-diversity-line-yellow"  /><span class="ubermenu-target-title ubermenu-target-text">Ethics &#038; Diversity</span></a></li><li id="menu-item-19897" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-has-children ubermenu-item-19897 ubermenu-item-level-0 ubermenu-column ubermenu-column-auto ubermenu-has-submenu-drop ubermenu-has-submenu-flyout ubermenu-flyout-full-height" ><a class="ubermenu-target ubermenu-target-with-image ubermenu-item-layout-default ubermenu-item-layout-image_left" href="https://aaai.org/aaai-conferences-and-symposia/" tabindex="0"><img class="ubermenu-image ubermenu-image-size-full" src="https://aaai.org/wp-content/uploads/2023/03/aaai-icon_conferences-symposia-line-yellow.png" width="40" height="40" alt="Conference talk bubble"  /><span class="ubermenu-target-title ubermenu-target-text">Conferences &#038; Symposia</span><i class='ubermenu-sub-indicator fas fa-angle-down'></i></a><ul  class="ubermenu-submenu ubermenu-submenu-id-19897 ubermenu-submenu-type-flyout ubermenu-submenu-drop ubermenu-submenu-align-vertical_full_height"  ><li id="menu-item-61221" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-61221 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="/conference/aaai/"><span class="ubermenu-target-title ubermenu-target-text">AAAI Conference</span></a></li><li id="menu-item-61222" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-61222 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="/conference/aies/"><span class="ubermenu-target-title ubermenu-target-text">AIES AAAI/ACM</span></a></li><li id="menu-item-61223" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-61223 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/conference/aiide-2/"><span class="ubermenu-target-title ubermenu-target-text">AIIDE</span></a></li><li id="menu-item-90208" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-90208 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/conference/eaai/"><span class="ubermenu-target-title ubermenu-target-text">EAAI</span></a></li><li id="menu-item-61226" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-61226 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="/conference/hcomp/"><span class="ubermenu-target-title ubermenu-target-text">HCOMP</span></a></li><li id="menu-item-61225" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-61225 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="/conference/iaai/"><span class="ubermenu-target-title ubermenu-target-text">IAAI</span></a></li><li id="menu-item-61224" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-61224 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/conference/icwsm/"><span class="ubermenu-target-title ubermenu-target-text">ICWSM</span></a></li><li id="menu-item-61227" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-61227 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="/conference/spring-symposia/"><span class="ubermenu-target-title ubermenu-target-text">Spring Symposia</span></a></li><li id="menu-item-61228" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-61228 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/conference/summer-symposia/"><span class="ubermenu-target-title ubermenu-target-text">Summer Symposia</span></a></li><li id="menu-item-61229" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-61229 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="/conference/fall-symposia/"><span class="ubermenu-target-title ubermenu-target-text">Fall Symposia</span></a></li><li id="menu-item-40935" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-40935 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/aaai-conferences-and-symposia/aaai-code-of-conduct-for-conferences-and-events/"><span class="ubermenu-target-title ubermenu-target-text">Code of Conduct for Conferences and Events</span></a></li></ul></li><li id="menu-item-230" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-has-children ubermenu-item-230 ubermenu-item-level-0 ubermenu-column ubermenu-column-auto ubermenu-has-submenu-drop ubermenu-has-submenu-flyout ubermenu-flyout-full-height" ><a class="ubermenu-target ubermenu-target-with-image ubermenu-item-layout-default ubermenu-item-layout-image_left" href="/aaai-publications/" tabindex="0"><img class="ubermenu-image ubermenu-image-size-full" src="https://aaai.org/wp-content/uploads/2023/03/aaai-icon_publications-line-yellow.png" width="40" height="40" alt="Publications"  /><span class="ubermenu-target-title ubermenu-target-text">Publications</span><i class='ubermenu-sub-indicator fas fa-angle-down'></i></a><ul  class="ubermenu-submenu ubermenu-submenu-id-230 ubermenu-submenu-type-flyout ubermenu-submenu-drop ubermenu-submenu-align-vertical_full_height"  ><li id="menu-item-61218" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-61218 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://onlinelibrary.wiley.com/journal/23719621"><span class="ubermenu-target-title ubermenu-target-text">AI Magazine</span></a></li><li id="menu-item-61219" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-61219 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/aaai-publications/aaai-conference-proceedings/"><span class="ubermenu-target-title ubermenu-target-text">Conference Proceedings</span></a></li><li id="menu-item-40932" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-40932 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/aaai-publications/aaai-publication-policies-guidelines/"><span class="ubermenu-target-title ubermenu-target-text">AAAI Publication Policies &#038; Guidelines</span></a></li><li id="menu-item-40928" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-40928 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/aaai-publications/request-to-reproduce-copyrighted-materials/"><span class="ubermenu-target-title ubermenu-target-text">Request to Reproduce Copyrighted Materials</span></a></li><li id="menu-item-83926" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-83926 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/aaai-publications/contribute/"><span class="ubermenu-target-title ubermenu-target-text">Contribute</span></a></li><li id="menu-item-61220" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-61220 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://www.proceedings.com/association-for-the-advancement-of-artificial-intelligence-aaai/"><span class="ubermenu-target-title ubermenu-target-text">Order Proceedings</span></a></li></ul></li><li id="menu-item-40060" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-40060 ubermenu-item-level-0 ubermenu-column ubermenu-column-auto" ><a class="ubermenu-target ubermenu-target-with-image ubermenu-item-layout-default ubermenu-item-layout-image_left" href="https://aaai.org/ai-magazine/" tabindex="0"><img class="ubermenu-image ubermenu-image-size-full" src="https://aaai.org/wp-content/uploads/2023/03/aaai-icon_ai-magazine-line-yellow.png" width="40" height="40" alt="aaai-icon_ai-magazine-line-yellow"  /><span class="ubermenu-target-title ubermenu-target-text">AI Magazine</span></a></li><li id="menu-item-727" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-has-children ubermenu-item-727 ubermenu-item-level-0 ubermenu-column ubermenu-column-auto ubermenu-has-submenu-drop ubermenu-has-submenu-flyout" ><a class="ubermenu-target ubermenu-target-with-image ubermenu-item-layout-default ubermenu-item-layout-image_left" href="https://aaai.org/membership/" tabindex="0"><img class="ubermenu-image ubermenu-image-size-full" src="https://aaai.org/wp-content/uploads/2023/03/aaai-icon_membership-line-yellow.png" width="40" height="40" alt="Membership"  /><span class="ubermenu-target-title ubermenu-target-text">Membership</span><i class='ubermenu-sub-indicator fas fa-angle-down'></i></a><ul  class="ubermenu-submenu ubermenu-submenu-id-727 ubermenu-submenu-type-flyout ubermenu-submenu-drop ubermenu-submenu-align-left_edge_item"  ><li id="menu-item-39825" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-39825 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" target="_blank" href="https://aaai.memberclicks.net/"><span class="ubermenu-target-title ubermenu-target-text">Member Login</span></a></li><li id="menu-item-729" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-729 ubermenu-item-auto ubermenu-item-normal ubermenu-item-level-1" ><a class="ubermenu-target ubermenu-item-layout-default ubermenu-item-layout-text_only" href="https://aaai.org/membership/aaai-chapter-program/"><span class="ubermenu-target-title ubermenu-target-text">Chapters</span></a></li></ul></li><li class="ubermenu-divider"><hr/></li><li id="menu-item-518" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-518 ubermenu-item-level-0 ubermenu-column ubermenu-column-auto" ><a class="ubermenu-target ubermenu-target-with-image ubermenu-item-layout-default ubermenu-item-layout-image_left" href="https://careers.aaai.org/" tabindex="0"><img class="ubermenu-image ubermenu-image-size-full" src="https://aaai.org/wp-content/uploads/2023/03/aaai-icon_career-line-yellow.png" width="40" height="40" alt="Career Center"  /><span class="ubermenu-target-title ubermenu-target-text">AI Jobs</span></a></li><li id="menu-item-61189" class="ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-61189 ubermenu-item-level-0 ubermenu-column ubermenu-column-auto" ><a class="ubermenu-target ubermenu-target-with-image ubermenu-item-layout-default ubermenu-item-layout-image_left" target="_blank" href="https://aitopics.org/search" tabindex="0"><img class="ubermenu-image ubermenu-image-size-full" src="https://aaai.org/wp-content/uploads/2023/03/aaai-icon_ai-topics-line-yellow.png" width="40" height="40" alt="aaai-icon_ai-topics-line-yellow"  /><span class="ubermenu-target-title ubermenu-target-text">AITopics</span></a></li><li id="menu-item-39821" class="ubermenu-item ubermenu-item-type-post_type ubermenu-item-object-page ubermenu-item-39821 ubermenu-item-level-0 ubermenu-column ubermenu-column-auto" ><a class="ubermenu-target ubermenu-target-with-image ubermenu-item-layout-default ubermenu-content-align-left ubermenu-item-layout-image_left ubermenu-noindicator" href="https://aaai.org/contact-aaai/" tabindex="0"><img class="ubermenu-image ubermenu-image-size-full" src="https://aaai.org/wp-content/uploads/2023/03/aaai-icon_contact-line-yellow.png" width="40" height="40" alt="aaai-icon_contact-line-yellow"  /><span class="ubermenu-target-title ubermenu-target-text">Contact</span></a></li><li class="ubermenu-divider"><hr/></li><li id="menu-item-75334" class="hidden-text ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-75334 ubermenu-item-level-0 ubermenu-column ubermenu-column-1-3 ubermenu-item-mini" ><a class="ubermenu-target ubermenu-target-with-icon ubermenu-item-layout-default ubermenu-item-layout-icon_left" href="https://twitter.com/RealAAAI" tabindex="0"><i class="ubermenu-icon fab fa-twitter-square"  title="twitter"></i><span class="ubermenu-target-title ubermenu-target-text">Twitter</span></a></li><li id="menu-item-75336" class="hidden-text ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-75336 ubermenu-item-level-0 ubermenu-column ubermenu-column-1-3 ubermenu-item-mini" ><a class="ubermenu-target ubermenu-target-with-icon ubermenu-item-layout-default ubermenu-item-layout-icon_left" href="https://www.facebook.com/AAAIOrg/" tabindex="0"><i class="ubermenu-icon fab fa-facebook-square"  title="Facebook"></i><span class="ubermenu-target-title ubermenu-target-text">Facebook</span></a></li><li id="menu-item-75333" class="hidden-text ubermenu-item ubermenu-item-type-custom ubermenu-item-object-custom ubermenu-item-75333 ubermenu-item-level-0 ubermenu-column ubermenu-column-1-3 ubermenu-item-mini" ><a class="ubermenu-target ubermenu-target-with-icon ubermenu-item-layout-default ubermenu-item-layout-icon_left" href="https://www.linkedin.com/company/association-for-the-advancement-of-artificial-intelligence-aaai-/" tabindex="0"><i class="ubermenu-icon fab fa-linkedin"  title="linkedin"></i><span class="ubermenu-target-title ubermenu-target-text">LinkedIn</span></a></li></ul></nav>
<!-- End UberMenu -->
	</div>
<div class="site-inner"><div class="content-sidebar-wrap"><main class="content" id="genesis-content"><div class="proceeding-breadcrumb"><a href="https://aaai.org/">Home </a> / <a href="https://aaai.org/aaai-publications/aaai-conference-proceedings/">Proceedings </a> / <a href="https://aaai.org/proceeding/aaai-34-2020/">Proceedings of the AAAI Conference on Artificial Intelligence, 34</a> / </div><div class="archive-description taxonomy-archive-description taxonomy-description"><h1 class="archive-title">Vol. 34 No. 07: AAAI-20 Technical Tracks 7</h1></div><div class="track-wrap"><h2>AAAI Technical Track: Vision</h2><ul><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13041-unified-vision-language-pre-training-for-image-captioning-and-vqa/">Unified Vision-Language Pre-Training for Image Captioning and VQA</a></h5><span class="papers-author-page"><p>Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, Jianfeng Gao</p><p>13041-13049</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7005/7005-13-10234-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13050-ladder-loss-for-coherent-visual-semantic-embedding/">Ladder Loss for Coherent Visual-Semantic Embedding</a></h5><span class="papers-author-page"><p>Mo Zhou, Zhenxing Niu, Le Wang, Zhanning Gao, Qilin Zhang, Gang Hua</p><p>13050-13057</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7006/7006-13-10235-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13058-generate-segment-and-refine-towards-generic-manipulation-segmentation/">Generate, Segment, and Refine: Towards Generic Manipulation Segmentation</a></h5><span class="papers-author-page"><p>Peng Zhou, Bor-Chun Chen, Xintong Han, Mahyar Najibi, Abhinav Shrivastava, Ser-Nam Lim, Larry Davis</p><p>13058-13065</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7007/7007-13-10236-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13066-motion-attentive-transition-for-zero-shot-video-object-segmentation/">Motion-Attentive Transition for Zero-Shot Video Object Segmentation</a></h5><span class="papers-author-page"><p>Tianfei Zhou, Shunzhou Wang, Yi Zhou, Yazhou Yao, Jianwu Li, Ling Shao</p><p>13066-13073</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7008/7008-13-10237-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13074-when-awgn-based-denoiser-meets-real-noises/">When AWGN-Based Denoiser Meets Real Noises</a></h5><span class="papers-author-page"><p>Yuqian Zhou, Jianbo Jiao, Haibin Huang, Yang Wang, Jue Wang, Honghui Shi, Thomas Huang</p><p>13074-13081</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7009/7009-13-10238-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13082-multi-type-self-attention-guided-degraded-saliency-detection/">Multi-Type Self-Attention Guided Degraded Saliency Detection</a></h5><span class="papers-author-page"><p>Ziqi Zhou, Zheng Wang, Huchuan Lu, Song Wang, Meijun Sun</p><p>13082-13089</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7010/7010-13-10239-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13090-towards-omni-supervised-face-alignment-for-large-scale-unlabeled-videos/">Towards Omni-Supervised Face Alignment for Large Scale Unlabeled Videos</a></h5><span class="papers-author-page"><p>Congcong Zhu, Hao Liu*(corresponding author), Zhenhua Yu, Xuehong Sun</p><p>13090-13097</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7011/7011-13-10240-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13098-faster-recurrent-networks-for-efficient-video-classification/">FASTER Recurrent Networks for Efficient Video Classification</a></h5><span class="papers-author-page"><p>Linchao Zhu, Du Tran, Laura Sevilla-Lara, Yi Yang, Matt Feiszli, Heng Wang</p><p>13098-13105</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7012/7012-13-10241-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13106-eemefn-low-light-image-enhancement-via-edge-enhanced-multi-exposure-fusion-network/">EEMEFN: Low-Light Image Enhancement via Edge-Enhanced Multi-Exposure Fusion Network</a></h5><span class="papers-author-page"><p>Minfeng Zhu, Pingbo Pan, Wei Chen, Yi Yang</p><p>13106-13113</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7013/7013-13-10242-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13114-viewpoint-aware-loss-with-angular-regularization-for-person-re-identification/">Viewpoint-Aware Loss with Angular Regularization for Person Re-Identification</a></h5><span class="papers-author-page"><p>Zhihui Zhu, Xinyang Jiang, Feng Zheng, Xiaowei Guo, Feiyue Huang, Xing Sun, Weishi Zheng</p><p>13114-13121</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7014/7014-13-10243-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13122-ifan-image-instance-full-alignment-networks-for-adaptive-object-detection/">iFAN: Image-Instance Full Alignment Networks for Adaptive Object Detection</a></h5><span class="papers-author-page"><p>Chenfan Zhuang, Xintong Han, Weilin Huang, Matthew Scott</p><p>13122-13129</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7015/7015-13-10244-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13130-learning-attentive-pairwise-interaction-for-fine-grained-classification/">Learning Attentive Pairwise Interaction for Fine-Grained Classification</a></h5><span class="papers-author-page"><p>Peiqin Zhuang, Yali Wang, Yu Qiao</p><p>13130-13137</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7016/7016-13-10245-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12878-single-camera-training-for-person-re-identification/">Single Camera Training for Person Re-Identification</a></h5><span class="papers-author-page"><p>Tianyu Zhang, Lingxi Xie, Longhui Wei, Yongfei Zhang, Bo Li, Qi Tian</p><p>12878-12885</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6985/6985-13-10214-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12886-multi-instance-multi-label-action-recognition-and-localization-based-on-spatio-temporal-pre-trimming-for-untrimmed-videos/">Multi-Instance Multi-Label Action Recognition and Localization Based on Spatio-Temporal Pre-Trimming for Untrimmed Videos</a></h5><span class="papers-author-page"><p>Xiao-Yu Zhang, Haichao Shi, Changsheng Li, Peng Li</p><p>12886-12893</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6986/6986-13-10215-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12894-fact-fused-attention-for-clothing-transfer-with-generative-adversarial-networks/">FACT: Fused Attention for Clothing Transfer with Generative Adversarial Networks</a></h5><span class="papers-author-page"><p>Yicheng Zhang, Lei Li, Li Song, Rong Xie, Wenjun Zhang</p><p>12894-12901</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6987/6987-13-10216-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12902-find-objects-and-focus-on-highlights-mining-object-semantics-for-video-highlight-detection-via-graph-neural-networks/">Find Objects and Focus on Highlights: Mining Object Semantics for Video Highlight Detection via Graph Neural Networks</a></h5><span class="papers-author-page"><p>Yingying Zhang, Junyu Gao, Xiaoshan Yang, Chang Liu, Yan Li, Changsheng Xu</p><p>12902-12909</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6988/6988-13-10217-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12910-when-radiology-report-generation-meets-knowledge-graph/">When Radiology Report Generation Meets Knowledge Graph</a></h5><span class="papers-author-page"><p>Yixiao Zhang, Xiaosong Wang, Ziyue Xu, Qihang Yu, Alan Yuille, Daguang Xu</p><p>12910-12917</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6989/6989-13-10218-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12918-exploiting-motion-information-from-unlabeled-videos-for-static-image-action-recognition/">Exploiting Motion Information from Unlabeled Videos for Static Image Action Recognition</a></h5><span class="papers-author-page"><p>Yiyi Zhang, Li Niu, Ziqi Pan, Meichao Luo, Jianfu Zhang, Dawei Cheng, Liqing Zhang</p><p>12918-12925</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6990/6990-13-10219-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12926-adaptive-unimodal-cost-volume-filtering-for-deep-stereo-matching/">Adaptive Unimodal Cost Volume Filtering for Deep Stereo Matching</a></h5><span class="papers-author-page"><p>Youmin Zhang, Yimin Chen, Xiao Bai, Suihanjin Yu, Kun Yu, Zhiwei Li, Kuiyuan Yang</p><p>12926-12934</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6991/6991-13-10220-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12935-fully-convolutional-network-for-consistent-voxel-wise-correspondence/">Fully Convolutional Network for Consistent Voxel-Wise Correspondence</a></h5><span class="papers-author-page"><p>Yungeng Zhang, Yuru Pei, Yuke Guo, Gengyu Ma, Tianmin Xu, Hongbin Zha</p><p>12935-12942</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6992/6992-13-10221-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12943-zero-shot-sketch-based-image-retrieval-via-graph-convolution-network/">Zero-Shot Sketch-Based Image Retrieval via Graph Convolution Network</a></h5><span class="papers-author-page"><p>Zhaolong Zhang, Yuejie Zhang, Rui Feng, Tao Zhang, Weiguo Fan</p><p>12943-12950</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6993/6993-13-10222-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12951-jsnet-joint-instance-and-semantic-segmentation-of-3d-point-clouds/">JSNet: Joint Instance and Semantic Segmentation of 3D Point Clouds</a></h5><span class="papers-author-page"><p>Lin Zhao, Wenbing Tao</p><p>12951-12958</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6994/6994-13-10223-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12959-spherical-criteria-for-fast-and-accurate-360-object-detection/">Spherical Criteria for Fast and Accurate 360° Object Detection</a></h5><span class="papers-author-page"><p>Pengyu Zhao, Ansheng You, Yuanxing Zhang, Jiaying Liu, Kaigui Bian, Yunhai Tong</p><p>12959-12966</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6995/6995-13-10224-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12967-gtnet-generative-transfer-network-for-zero-shot-object-detection/">GTNet: Generative Transfer Network for Zero-Shot Object Detection</a></h5><span class="papers-author-page"><p>Shizhen Zhao, Changxin Gao, Yuanjie Shao, Lerenhan Li, Changqian Yu, Zhong Ji, Nong Sang</p><p>12967-12974</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6996/6996-13-10225-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12975-multi-source-distilling-domain-adaptation/">Multi-Source Distilling Domain Adaptation</a></h5><span class="papers-author-page"><p>Sicheng Zhao, Guangzhi Wang, Shanghang Zhang, Yang Gu, Yaxian Li, Zhichao Song, Pengfei Xu, Runbo Hu, Hua Chai, Kurt Keutzer</p><p>12975-12983</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6997/6997-13-10226-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12984-memcap-memorizing-style-knowledge-for-image-captioning/">MemCap: Memorizing Style Knowledge for Image Captioning</a></h5><span class="papers-author-page"><p>Wentian Zhao, Xinxiao Wu, Xiaoxun Zhang</p><p>12984-12992</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6998/6998-13-10227-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12993-distance-iou-loss-faster-and-better-learning-for-bounding-box-regression/">Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression</a></h5><span class="papers-author-page"><p>Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, Dongwei Ren</p><p>12993-13000</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6999/6999-13-10228-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13001-random-erasing-data-augmentation/">Random Erasing Data Augmentation</a></h5><span class="papers-author-page"><p>Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang</p><p>13001-13008</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7000/7000-13-10229-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13009-spatial-temporal-multi-cue-network-for-continuous-sign-language-recognition/">Spatial-Temporal Multi-Cue Network for Continuous Sign Language Recognition</a></h5><span class="papers-author-page"><p>Hao Zhou, Wengang Zhou, Yun Zhou, Houqiang Li</p><p>13009-13016</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7001/7001-13-10230-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13017-discriminative-and-robust-online-learning-for-siamese-visual-tracking/">Discriminative and Robust Online Learning for Siamese Visual Tracking</a></h5><span class="papers-author-page"><p>Jinghao Zhou, Peng Wang, Haoyang Sun</p><p>13017-13024</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7002/7002-13-10231-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13025-deep-domain-adversarial-image-generation-for-domain-generalisation/">Deep Domain-Adversarial Image Generation for Domain Generalisation</a></h5><span class="papers-author-page"><p>Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, Tao Xiang</p><p>13025-13032</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7003/7003-13-10232-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/13033-progressive-bi-c3d-pose-grammar-for-human-pose-estimation/">Progressive Bi-C3D Pose Grammar for Human Pose Estimation</a></h5><span class="papers-author-page"><p>Lu Zhou, Yingying Chen, Jinqiao Wang, Hanqing Lu</p><p>13033-13040</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/7004/7004-13-10233-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12717-pointwise-rotation-invariant-network-with-adaptive-sampling-and-3d-spherical-voxel-convolution/">Pointwise Rotation-Invariant Network with Adaptive Sampling and 3D Spherical Voxel Convolution</a></h5><span class="papers-author-page"><p>Yang You, Yujing Lou, Qi Liu, Yu-Wing Tai, Lizhuang Ma, Cewu Lu, Weiming Wang</p><p>12717-12724</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6965/6965-13-10194-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12725-cascading-convolutional-color-constancy/">Cascading Convolutional Color Constancy</a></h5><span class="papers-author-page"><p>Huanglin Yu, Ke Chen, Kaiqi Wang, Yanlin Qian, Zhaoxiang Zhang, Kui Jia</p><p>12725-12732</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6966/6966-13-10195-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12733-region-normalization-for-image-inpainting/">Region Normalization for Image Inpainting</a></h5><span class="papers-author-page"><p>Tao Yu, Zongyu Guo, Xin Jin, Shilin Wu, Zhibo Chen, Weiping Li, Zhizheng Zhang, Sen Liu</p><p>12733-12740</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6967/6967-13-10196-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12741-patchy-image-structure-classification-using-multi-orientation-region-transform/">Patchy Image Structure Classification Using Multi-Orientation Region Transform</a></h5><span class="papers-author-page"><p>Xiaohan Yu, Yang Zhao, Yongsheng Gao, Shengwu Xiong, Xiaohui Yuan</p><p>12741-12748</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6968/6968-13-10197-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12749-human-synthesis-and-scene-compositing/">Human Synthesis and Scene Compositing</a></h5><span class="papers-author-page"><p>Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Andrei Zanfir, Cristian Sminchisescu</p><p>12749-12756</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6969/6969-13-10198-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12757-realistic-face-reenactment-via-self-supervised-disentangling-of-identity-and-pose/">Realistic Face Reenactment via Self-Supervised Disentangling of Identity and Pose</a></h5><span class="papers-author-page"><p>Xianfang Zeng, Yusu Pan, Mengmeng Wang, Jiangning Zhang, Yong Liu</p><p>12757-12764</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6970/6970-13-10199-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12765-reliability-does-matter-an-end-to-end-weakly-supervised-semantic-segmentation-approach/">Reliability Does Matter: An End-to-End Weakly Supervised Semantic Segmentation Approach</a></h5><span class="papers-author-page"><p>Bingfeng Zhang, Jimin Xiao, Yunchao Wei, Mingjie Sun, Kaizhu Huang</p><p>12765-12772</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6971/6971-13-10200-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12773-shape-oriented-convolution-neural-network-for-point-cloud-analysis/">Shape-Oriented Convolution Neural Network for Point Cloud Analysis</a></h5><span class="papers-author-page"><p>Chaoyi Zhang, Yang Song, Lina Yao, Weidong Cai</p><p>12773-12780</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6972/6972-13-10201-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12781-web-supervised-network-with-softly-update-drop-training-for-fine-grained-visual-classification/">Web-Supervised Network with Softly Update-Drop Training for Fine-Grained Visual Classification</a></h5><span class="papers-author-page"><p>Chuanyi Zhang, Yazhou Yao, Huafeng Liu, Guo-Sen Xie, Xiangbo Shu, Tianfei Zhou, Zheng Zhang, Fumin Shen, Zhenmin Tang</p><p>12781-12788</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6973/6973-13-10202-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12789-fdn-feature-decoupling-network-for-head-pose-estimation/">FDN: Feature Decoupling Network for Head Pose Estimation</a></h5><span class="papers-author-page"><p>Hao Zhang, Mengmeng Wang, Yong Liu, Yi Yuan</p><p>12789-12796</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6974/6974-13-10203-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12797-rethinking-the-image-fusion-a-fast-unified-image-fusion-network-based-on-proportional-maintenance-of-gradient-and-intensity/">Rethinking the Image Fusion: A Fast Unified Image Fusion Network based on Proportional Maintenance of Gradient and Intensity</a></h5><span class="papers-author-page"><p>Hao Zhang, Han Xu, Yang Xiao, Xiaojie Guo, Jiayi Ma</p><p>12797-12804</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6975/6975-13-10204-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12805-model-watermarking-for-image-processing-networks/">Model Watermarking for Image Processing Networks</a></h5><span class="papers-author-page"><p>Jie Zhang, Dongdong Chen, Jing Liao, Han Fang, Weiming Zhang, Wenbo Zhou, Hao Cui, Nenghai Yu</p><p>12805-12812</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6976/6976-13-10205-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12813-deep-object-co-segmentation-via-spatial-semantic-network-modulation/">Deep Object Co-Segmentation via Spatial-Semantic Network Modulation</a></h5><span class="papers-author-page"><p>Kaihua Zhang, Jin Chen, Bo Liu, Qingshan Liu</p><p>12813-12820</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6977/6977-13-10206-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12821-pixel-aware-deep-function-mixture-network-for-spectral-super-resolution/">Pixel-Aware Deep Function-Mixture Network for Spectral Super-Resolution</a></h5><span class="papers-author-page"><p>Lei Zhang, Zhiqiang Lang, Peng Wang, Wei Wei, Shengcai Liao, Ling Shao, Yanning Zhang</p><p>12821-12828</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6978/6978-13-10207-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12829-ris-gan-explore-residual-and-illumination-with-generative-adversarial-networks-for-shadow-removal/">RIS-GAN: Explore Residual and Illumination with Generative Adversarial Networks for Shadow Removal</a></h5><span class="papers-author-page"><p>Ling Zhang, Chengjiang Long, Xiaolong Zhang, Chunxia Xiao</p><p>12829-12836</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6979/6979-13-10208-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12837-3d-crowd-counting-via-multi-view-fusion-with-3d-gaussian-kernels/">3D Crowd Counting via Multi-View Fusion with 3D Gaussian Kernels</a></h5><span class="papers-author-page"><p>Qi Zhang, Antoni B. Chan</p><p>12837-12844</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6980/6980-13-10209-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12845-deep-camouflage-images/">Deep Camouflage Images</a></h5><span class="papers-author-page"><p>Qing Zhang, Gelin Yin, Yongwei Nie, Wei-Shi Zheng</p><p>12845-12852</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6981/6981-13-10210-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12853-autoremover-automatic-object-removal-for-autonomous-driving-videos/">AutoRemover: Automatic Object Removal for Autonomous Driving Videos</a></h5><span class="papers-author-page"><p>Rong Zhang, Wei Li, Peng Wang, Chenye Guan, Jin Fang, Yuhang Song, Jinhui Yu, Baoquan Chen, Weiwei Xu, Ruigang Yang</p><p>12853-12861</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6982/6982-13-10211-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12862-knowledge-integration-networks-for-action-recognition/">Knowledge Integration Networks for Action Recognition</a></h5><span class="papers-author-page"><p>Shiwen Zhang, Sheng Guo, Limin Wang, Weilin Huang, Matthew Scott</p><p>12862-12869</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6983/6983-13-10212-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12870-learning-2d-temporal-adjacent-networks-for-moment-localization-with-natural-language/">Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language</a></h5><span class="papers-author-page"><p>Songyang Zhang, Houwen Peng, Jianlong Fu, Jiebo Luo</p><p>12870-12877</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6984/6984-13-10213-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12557-zoomnet-part-aware-adaptive-zooming-neural-network-for-3d-object-detection/">ZoomNet: Part-Aware Adaptive Zooming Neural Network for 3D Object Detection</a></h5><span class="papers-author-page"><p>Zhenbo Xu, Wei Zhang, Xiaoqing Ye, Xiao Tan, Wei Yang, Shilei Wen, Errui Ding, Ajin Meng, Liusheng Huang</p><p>12557-12564</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6945/6945-13-10174-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12565-shape-aware-organ-segmentation-by-predicting-signed-distance-maps/">Shape-Aware Organ Segmentation by Predicting Signed Distance Maps</a></h5><span class="papers-author-page"><p>Yuan Xue, Hui Tang, Zhi Qiao, Guanzhong Gong, Yong Yin, Zhen Qian, Chao Huang, Wei Fan, Xiaolei Huang</p><p>12565-12572</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6946/6946-13-10175-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12573-fas-net-construct-effective-features-adaptively-for-multi-scale-object-detection/">FAS-Net: Construct Effective Features Adaptively for Multi-Scale Object Detection</a></h5><span class="papers-author-page"><p>Jiangqiao Yan, Yue Zhang, Zhonghan Chang, Tengfei Zhang, Menglong Yan, Wenhui Diao, Hongqi Wang, Xian Sun</p><p>12573-12580</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6947/6947-13-10176-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12581-gated-convolutional-networks-with-hybrid-connectivity-for-image-classification/">Gated Convolutional Networks with Hybrid Connectivity for Image Classification</a></h5><span class="papers-author-page"><p>Chuanguang Yang, Zhulin An, Hui Zhu, Xiaolong Hu, Kun Zhang, Kaiqiang Xu, Chao Li, Yongjun Xu</p><p>12581-12588</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6948/6948-13-10177-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12589-mining-on-heterogeneous-manifolds-for-zero-shot-cross-modal-image-retrieval/">Mining on Heterogeneous Manifolds for Zero-Shot Cross-Modal Image Retrieval</a></h5><span class="papers-author-page"><p>Fan Yang, Zheng Wang, Jing Xiao, Shin'ichi Satoh</p><p>12589-12596</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6949/6949-13-10178-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12597-asymmetric-co-teaching-for-unsupervised-cross-domain-person-re-identification/">Asymmetric Co-Teaching for Unsupervised Cross-Domain Person Re-Identification</a></h5><span class="papers-author-page"><p>Fengxiang Yang, Ke Li, Zhun Zhong, Zhiming Luo, Xing Sun, Hao Cheng, Xiaowei Guo, Feiyue Huang, Rongrong Ji, Shaozi Li</p><p>12597-12604</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6950/6950-13-10179-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12605-learning-to-incorporate-structure-knowledge-for-image-inpainting/">Learning to Incorporate Structure Knowledge for Image Inpainting</a></h5><span class="papers-author-page"><p>Jie Yang, Zhiquan Qi, Yong Shi</p><p>12605-12612</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6951/6951-13-10180-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12613-an-adversarial-perturbation-oriented-domain-adaptation-approach-for-semantic-segmentation/">An Adversarial Perturbation Oriented Domain Adaptation Approach for Semantic Segmentation</a></h5><span class="papers-author-page"><p>Jihan Yang, Ruijia Xu, Ruiyu Li, Xiaojuan Qi, Xiaoyong Shen, Guanbin Li, Liang Lin</p><p>12613-12620</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6952/6952-13-10181-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12621-fan-face-a-simple-orthogonal-improvement-to-deep-face-recognition/">FAN-Face: a Simple Orthogonal Improvement to Deep Face Recognition</a></h5><span class="papers-author-page"><p>Jing Yang, Adrian Bulat, Georgios Tzimiropoulos</p><p>12621-12628</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6953/6953-13-10182-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12629-towards-scale-free-rain-streak-removal-via-self-supervised-fractal-band-learning/">Towards Scale-Free Rain Streak Removal via Self-Supervised Fractal Band Learning</a></h5><span class="papers-author-page"><p>Wenhan Yang, Shiqi Wang, Dejia Xu, Xiaodong Wang, Jiaying Liu</p><p>12629-12636</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6954/6954-13-10183-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12637-sognet-scene-overlap-graph-network-for-panoptic-segmentation/">SOGNet: Scene Overlap Graph Network for Panoptic Segmentation</a></h5><span class="papers-author-page"><p>Yibo Yang, Hongyang Li, Xia Li, Qijie Zhao, Jianlong Wu, Zhouchen Lin</p><p>12637-12644</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6955/6955-13-10184-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12645-release-the-power-of-online-training-for-robust-visual-tracking/">Release the Power of Online-Training for Robust Visual Tracking</a></h5><span class="papers-author-page"><p>Yifan Yang, Guorong Li, Yuankai Qi, QIngming Huang</p><p>12645-12652</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6956/6956-13-10185-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12653-context-transformer-tackling-object-confusion-for-few-shot-detection/">Context-Transformer: Tackling Object Confusion for Few-Shot Detection</a></h5><span class="papers-author-page"><p>Ze Yang, Yali Wang, Xianyu Chen, Jianzhuang Liu, Yu Qiao</p><p>12653-12660</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6957/6957-13-10186-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12661-sm-nas-structural-to-modular-neural-architecture-search-for-object-detection/">SM-NAS: Structural-to-Modular Neural Architecture Search for Object Detection</a></h5><span class="papers-author-page"><p>Lewei Yao, Hang Xu, Wei Zhang, Xiaodan Liang, Zhenguo Li</p><p>12661-12668</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6958/6958-13-10187-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12669-deep-discriminative-cnn-with-temporal-ensembling-for-ambiguously-labeled-image-classification/">Deep Discriminative CNN with Temporal Ensembling for Ambiguously-Labeled Image Classification</a></h5><span class="papers-author-page"><p>Yao Yao, Jiehui Deng, Xiuhua Chen, Chen Gong, Jianxin Wu, Jian Yang</p><p>12669-12676</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6959/6959-13-10188-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12677-object-guided-instance-segmentation-for-biological-images/">Object-Guided Instance Segmentation for Biological Images</a></h5><span class="papers-author-page"><p>Jingru Yi, Hui Tang, Pengxiang Wu, Bo Liu, Daniel J. Hoeppner, Dimitris N. Metaxas, Lianyi Han, Wei Fan</p><p>12677-12684</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6960/6960-13-10189-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12685-leveraging-multi-view-image-sets-for-unsupervised-intrinsic-image-decomposition-and-highlight-separation/">Leveraging Multi-View Image Sets for Unsupervised Intrinsic Image Decomposition and Highlight Separation</a></h5><span class="papers-author-page"><p>Renjiao Yi, Ping Tan, Stephen Lin</p><p>12685-12692</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6961/6961-13-10190-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12693-joint-super-resolution-and-alignment-of-tiny-faces/">Joint Super-Resolution and Alignment of Tiny Faces</a></h5><span class="papers-author-page"><p>Yu Yin, Joseph Robinson, Yulun Zhang, Yun Fu</p><p>12693-12700</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6962/6962-13-10191-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12701-facial-action-unit-intensity-estimation-via-semantic-correspondence-learning-with-dynamic-graph-convolution/">Facial Action Unit Intensity Estimation via Semantic Correspondence Learning with Dynamic Graph Convolution</a></h5><span class="papers-author-page"><p>Yingruo Fan, Jacqueline Lam, Victor Li</p><p>12701-12708</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6963/6963-13-10192-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12709-cross-modality-attention-with-semantic-graph-embedding-for-multi-label-classification/">Cross-Modality Attention with Semantic Graph Embedding for Multi-Label Classification</a></h5><span class="papers-author-page"><p>Renchun You, Zhiyao Guo, Lei Cui, Xiang Long, Yingze Bao, Shilei Wen</p><p>12709-12716</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6964/6964-13-10193-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12394-distraction-aware-feature-learning-for-human-attribute-recognition-via-coarse-to-fine-attention-mechanism/">Distraction-Aware Feature Learning for Human Attribute Recognition via Coarse-to-Fine Attention Mechanism</a></h5><span class="papers-author-page"><p>Mingda Wu, Di Huang, Yuanfang Guo, Yunhong Wang</p><p>12394-12401</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6925/6925-13-10154-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12402-patch-proposal-network-for-fast-semantic-segmentation-of-high-resolution-images/">Patch Proposal Network for Fast Semantic Segmentation of High-Resolution Images</a></h5><span class="papers-author-page"><p>Tong Wu, Zhenzhen Lei, Bingqian Lin, Cuihua Li, Yanyun Qu, Yuan Xie</p><p>12402-12409</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6926/6926-13-10155-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12410-salsac-a-video-saliency-prediction-model-with-shuffled-attentions-and-correlation-based-convlstm/">SalSAC: A Video Saliency Prediction Model with Shuffled Attentions and Correlation-Based ConvLSTM</a></h5><span class="papers-author-page"><p>Xinyi Wu, Zhenyao Wu, Jinglin Zhang, Lili Ju, Song Wang</p><p>12410-12417</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6927/6927-13-10156-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12418-recognizing-instagram-filtered-images-with-feature-de-stylization/">Recognizing Instagram Filtered Images with Feature De-Stylization</a></h5><span class="papers-author-page"><p>Zhe Wu, Zuxuan Wu, Bharat Singh, Larry Davis</p><p>12418-12425</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6928/6928-13-10157-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12426-convolutional-hierarchical-attention-network-for-query-focused-video-summarization/">Convolutional Hierarchical Attention Network for Query-Focused Video Summarization</a></h5><span class="papers-author-page"><p>Shuwen Xiao, Zhou Zhao, Zijian Zhang, Xiaohui Yan, Min Yang</p><p>12426-12433</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6929/6929-13-10158-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12434-adversarial-learning-of-privacy-preserving-and-task-oriented-representations/">Adversarial Learning of Privacy-Preserving and Task-Oriented Representations</a></h5><span class="papers-author-page"><p>Taihong Xiao, Yi-Hsuan Tsai, Kihyuk Sohn, Manmohan Chandraker, Ming-Hsuan Yang</p><p>12434-12441</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6930/6930-13-10159-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12442-motion-based-generator-model-unsupervised-disentanglement-of-appearance-trackable-and-intrackable-motions-in-dynamic-patterns/">Motion-Based Generator Model: Unsupervised Disentanglement of Appearance, Trackable and Intrackable Motions in Dynamic Patterns</a></h5><span class="papers-author-page"><p>Jianwen Xie, Ruiqi Gao, Zilong Zheng, Song-Chun Zhu, Ying Nian Wu</p><p>12442-12451</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6931/6931-13-10160-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12452-segmenting-medical-mri-via-recurrent-decoding-cell/">Segmenting Medical MRI via Recurrent Decoding Cell</a></h5><span class="papers-author-page"><p>Ying Wen, Kai Xie, Lianghua He</p><p>12452-12459</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6932/6932-13-10161-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12460-pi-rcnn-an-efficient-multi-sensor-3d-object-detector-with-point-based-attentive-cont-conv-fusion-module/">PI-RCNN: An Efficient Multi-Sensor 3D Object Detector with Point-Based Attentive Cont-Conv Fusion Module</a></h5><span class="papers-author-page"><p>Liang Xie, Chao Xiang, Zhengxu Yu, Guodong Xu, Zheng Yang, Deng Cai, Xiaofei He</p><p>12460-12467</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6933/6933-13-10162-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12468-video-face-super-resolution-with-motion-adaptive-feedback-cell/">Video Face Super-Resolution with Motion-Adaptive Feedback Cell</a></h5><span class="papers-author-page"><p>Jingwei Xin, Nannan Wang, Jie Li, Xinbo Gao, Zhifeng Li</p><p>12468-12475</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6934/6934-13-10163-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12476-facial-attribute-capsules-for-noise-face-super-resolution/">Facial Attribute Capsules for Noise Face Super Resolution</a></h5><span class="papers-author-page"><p>Jingwei Xin, Nannan Wang, Xinrui Jiang, Jie Li, Xinbo Gao, Zhifeng Li</p><p>12476-12483</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6935/6935-13-10164-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12484-fusiondn-a-unified-densely-connected-network-for-image-fusion/">FusionDN: A Unified Densely Connected Network for Image Fusion</a></h5><span class="papers-author-page"><p>Han Xu, Jiayi Ma, Zhuliang Le, Junjun Jiang, Xiaojie Guo</p><p>12484-12491</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6936/6936-13-10165-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12492-universal-rcnn-universal-object-detector-via-transferable-graph-r-cnn/">Universal-RCNN: Universal Object Detector via Transferable Graph R-CNN</a></h5><span class="papers-author-page"><p>Hang Xu, Linpu Fang, Xiaodan Liang, Wenxiong Kang, Zhenguo Li</p><p>12492-12499</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6937/6937-13-10166-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12500-geometry-sharing-network-for-3d-point-cloud-classification-and-segmentation/">Geometry Sharing Network for 3D Point Cloud Classification and Segmentation</a></h5><span class="papers-author-page"><p>Mingye Xu, Zhipeng Zhou, Yu Qiao</p><p>12500-12507</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6938/6938-13-10167-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12508-learning-inverse-depth-regression-for-multi-view-stereo-with-correlation-cost-volume/">Learning Inverse Depth Regression for Multi-View Stereo with Correlation Cost Volume</a></h5><span class="papers-author-page"><p>Qingshan Xu, Wenbing Tao</p><p>12508-12515</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6939/6939-13-10168-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12516-planar-prior-assisted-patchmatch-multi-view-stereo/">Planar Prior Assisted PatchMatch Multi-View Stereo</a></h5><span class="papers-author-page"><p>Qingshan Xu, Wenbing Tao</p><p>12516-12523</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6940/6940-13-10169-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12524-a-proposal-based-approach-for-activity-image-to-video-retrieval/">A Proposal-Based Approach for Activity Image-to-Video Retrieval</a></h5><span class="papers-author-page"><p>Ruicong Xu, Li Niu, Jianfu Zhang, Liqing Zhang</p><p>12524-12531</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6941/6941-13-10170-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12532-gdface-gated-deformation-for-multi-view-face-image-synthesis/">GDFace: Gated Deformation for Multi-View Face Image Synthesis</a></h5><span class="papers-author-page"><p>Xuemiao Xu, Keke Li, Cheng Xu, Shengfeng He</p><p>12532-12540</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6942/6942-13-10171-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12541-cf-lstm-cascaded-feature-based-long-short-term-networks-for-predicting-pedestrian-trajectory/">CF-LSTM: Cascaded Feature-Based Long Short-Term Networks for Predicting Pedestrian Trajectory</a></h5><span class="papers-author-page"><p>Yi Xu, Jing Yang, Shaoyi Du</p><p>12541-12548</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6943/6943-13-10172-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12549-siamfc-towards-robust-and-accurate-visual-tracking-with-target-estimation-guidelines/">SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines</a></h5><span class="papers-author-page"><p>Yinda Xu, Zeyu Wang, Zuoxin Li, Ye Yuan, Gang Yu</p><p>12549-12556</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6944/6944-13-10173-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12233-consistent-video-style-transfer-via-compound-regularization/">Consistent Video Style Transfer via Compound Regularization</a></h5><span class="papers-author-page"><p>Wenjing Wang, Jizheng Xu, Li Zhang, Yue Wang, Jiaying Liu</p><p>12233-12240</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6905/6905-13-10134-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12241-mis-classified-vector-guided-softmax-loss-for-face-recognition/">Mis-Classified Vector Guided Softmax Loss for Face Recognition</a></h5><span class="papers-author-page"><p>Xiaobo Wang, Shifeng Zhang, Shuo Wang, Tianyu Fu, Hailin Shi, Tao Mei</p><p>12241-12248</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6906/6906-13-10135-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12249-symbiotic-attention-with-privileged-information-for-egocentric-action-recognition/">Symbiotic Attention with Privileged Information for Egocentric Action Recognition</a></h5><span class="papers-author-page"><p>Xiaohan Wang, Yu Wu, Linchao Zhu, Yi Yang</p><p>12249-12256</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6907/6907-13-10136-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12257-task-aware-monocular-depth-estimation-for-3d-object-detection/">Task-Aware Monocular Depth Estimation for 3D Object Detection</a></h5><span class="papers-author-page"><p>Xinlong Wang, Wei Yin, Tao Kong, Yuning Jiang, Lei Li, Chunhua Shen</p><p>12257-12264</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6908/6908-13-10137-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12265-multi-label-classification-with-label-graph-superimposing/">Multi-Label Classification with Label Graph Superimposing</a></h5><span class="papers-author-page"><p>Ya Wang, Dongliang He, Fu Li, Xiang Long, Zhichao Zhou, Jinwen Ma, Shilei Wen</p><p>12265-12272</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6909/6909-13-10138-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12273-pruning-from-scratch/">Pruning from Scratch</a></h5><span class="papers-author-page"><p>Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, Hang Su, Bo Zhang, Xiaolin Hu</p><p>12273-12280</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6910/6910-13-10139-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12281-learning-diverse-stochastic-human-action-generators-by-learning-smooth-latent-transitions/">Learning Diverse Stochastic Human-Action Generators by Learning Smooth Latent Transitions</a></h5><span class="papers-author-page"><p>Zhenyi Wang, Ping Yu, Yang Zhao, Ruiyi Zhang, Yufan Zhou, Junsong Yuan, Changyou Chen</p><p>12281-12288</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6911/6911-13-10140-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12289-graph-propagation-based-correlation-learning-for-weakly-supervised-fine-grained-image-classification/">Graph-Propagation Based Correlation Learning for Weakly Supervised Fine-Grained Image Classification</a></h5><span class="papers-author-page"><p>Zhuhui Wang, Shijie Wang, Haojie Li, Zhi Dou, Jianjun Li</p><p>12289-12296</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6912/6912-13-10141-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12297-localize-assemble-and-predicate-contextual-object-proposal-embedding-for-visual-relation-detection/">Localize, Assemble, and Predicate: Contextual Object Proposal Embedding for Visual Relation Detection</a></h5><span class="papers-author-page"><p>Ruihai Wu, Kehan Xu, Chenchen Liu, Nan Zhuang, Yadong Mu</p><p>12297-12304</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6913/6913-13-10142-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12305-efanet-exchangeable-feature-alignment-network-for-arbitrary-style-transfer/">EFANet: Exchangeable Feature Alignment Network for Arbitrary Style Transfer</a></h5><span class="papers-author-page"><p>Zhijie Wu, Chunjin Song, Yang Zhou, Minglun Gong, Hui Huang</p><p>12305-12312</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6914/6914-13-10143-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12313-adaptive-cross-modal-embeddings-for-image-text-alignment/">Adaptive Cross-Modal Embeddings for Image-Text Alignment</a></h5><span class="papers-author-page"><p>Jonatas Wehrmann, Camila Kolling, Rodrigo C Barros</p><p>12313-12320</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6915/6915-13-10144-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12321-f%c2%b3net-fusion-feedback-and-focus-for-salient-object-detection/">F³Net: Fusion, Feedback and Focus for Salient Object Detection</a></h5><span class="papers-author-page"><p>Jun Wei, Shuhui Wang, Qingming Huang</p><p>12321-12328</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6916/6916-13-10145-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12329-3d-single-person-concurrent-activity-detection-using-stacked-relation-network/">3D Single-Person Concurrent Activity Detection Using Stacked Relation Network</a></h5><span class="papers-author-page"><p>Yi Wei, Wenbo Li, Yanbo Fan, Linghan Xu, Ming-Ching Chang, Siwei Lyu</p><p>12329-12337</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6917/6917-13-10146-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12338-heuristic-black-box-adversarial-attacks-on-video-recognition-models/">Heuristic Black-Box Adversarial Attacks on Video Recognition Models</a></h5><span class="papers-author-page"><p>Zhipeng Wei, Jingjing Chen, Xingxing Wei, Linxi Jiang, Tat-Seng Chua, Fengfeng Zhou, Yu-Gang Jiang</p><p>12338-12345</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6918/6918-13-10147-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12346-efficient-querying-from-weighted-binary-codes/">Efficient Querying from Weighted Binary Codes</a></h5><span class="papers-author-page"><p>Zhenyu Weng, Yuesheng Zhu</p><p>12346-12353</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6919/6919-13-10148-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12354-online-hashing-with-efficient-updating-of-binary-codes/">Online Hashing with Efficient Updating of Binary Codes</a></h5><span class="papers-author-page"><p>Zhenyu Weng, Yuesheng Zhu</p><p>12354-12361</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6920/6920-13-10149-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12362-tracklet-self-supervised-learning-for-unsupervised-person-re-identification/">Tracklet Self-Supervised Learning for Unsupervised Person Re-Identification</a></h5><span class="papers-author-page"><p>Guile Wu, Xiatian Zhu, Shaogang Gong</p><p>12362-12369</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6921/6921-13-10150-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12370-circlenet-for-hip-landmark-detection/">CircleNet for Hip Landmark Detection</a></h5><span class="papers-author-page"><p>Hai Wu, Hongtao Xie, Chuanbin Liu, Zheng-Jun Zha, Jun Sun, Yongdong Zhang</p><p>12370-12377</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6922/6922-13-10151-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12378-3d-human-pose-estimation-via-explicit-compositional-depth-maps/">3D Human Pose Estimation via Explicit Compositional Depth Maps</a></h5><span class="papers-author-page"><p>Haiping Wu, Bin Xiao</p><p>12378-12385</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6923/6923-13-10152-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12386-tree-structured-policy-based-progressive-reinforcement-learning-for-temporally-language-grounding-in-video/">Tree-Structured Policy Based Progressive Reinforcement Learning for Temporally Language Grounding in Video</a></h5><span class="papers-author-page"><p>Jie Wu, Guanbin Li, Si Liu, Liang Lin</p><p>12386-12393</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6924/6924-13-10153-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12071-v-prom-a-benchmark-for-visual-reasoning-using-visual-progressive-matrices/">V-PROM: A Benchmark for Visual Reasoning Using Visual Progressive Matrices</a></h5><span class="papers-author-page"><p>Damien Teney, Peng Wang, Jiewei Cao, Lingqiao Liu, Chunhua Shen, Anton van den Hengel</p><p>12071-12078</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6885/6885-13-10114-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12079-end-to-end-thorough-body-perception-for-person-search/">End-to-End Thorough Body Perception for Person Search</a></h5><span class="papers-author-page"><p>Kun Tian, Houjing Huang, Yun Ye, Shiyu Li, Jinbin Lin, Guan Huang</p><p>12079-12086</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6886/6886-13-10115-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12087-differentiable-meta-learning-model-for-few-shot-semantic-segmentation/">Differentiable Meta-Learning Model for Few-Shot Semantic Segmentation</a></h5><span class="papers-author-page"><p>Pinzhuo Tian, Zhangkai Wu, Lei Qi, Lei Wang, Yinghuan Shi, Yang Gao</p><p>12087-12094</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6887/6887-13-10116-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12095-attention-based-view-selection-networks-for-light-field-disparity-estimation/">Attention-Based View Selection Networks for Light-Field Disparity Estimation</a></h5><span class="papers-author-page"><p>Yu-Ju Tsai, Yu-Lun Liu, Ming Ouhyoung, Yung-Yu Chuang</p><p>12095-12103</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6888/6888-13-10117-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12104-image-cropping-with-composition-and-saliency-aware-aesthetic-score-map/">Image Cropping with Composition and Saliency Aware Aesthetic Score Map</a></h5><span class="papers-author-page"><p>Yi Tu, Li Niu, Weijie Zhao, Dawei Cheng, Liqing Zhang</p><p>12104-12111</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6889/6889-13-10118-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12112-optical-flow-in-deep-visual-tracking/">Optical Flow in Deep Visual Tracking</a></h5><span class="papers-author-page"><p>Mikko Vihlman, Arto Visala</p><p>12112-12119</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6890/6890-13-10119-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12120-textscanner-reading-characters-in-order-for-robust-scene-text-recognition/">TextScanner: Reading Characters in Order for Robust Scene Text Recognition</a></h5><span class="papers-author-page"><p>Zhaoyi Wan, Minghang He, Haoran Chen, Xiang Bai, Cong Yao</p><p>12120-12127</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6891/6891-13-10120-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12128-progressive-feature-polishing-network-for-salient-object-detection/">Progressive Feature Polishing Network for Salient Object Detection</a></h5><span class="papers-author-page"><p>Bo Wang, Quan Chen, Min Zhou, Zhiqiang Zhang, Xiaogang Jin, Kun Gai</p><p>12128-12135</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6892/6892-13-10121-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12136-region-based-global-reasoning-networks/">Region-Based Global Reasoning Networks</a></h5><span class="papers-author-page"><p>Chuanming Wang, Huiyuan Fu, Charles X. Ling, Peilun Du, Huadong Ma</p><p>12136-12143</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6893/6893-13-10122-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12144-cross-modality-paired-images-generation-for-rgb-infrared-person-re-identification/">Cross-Modality Paired-Images Generation for RGB-Infrared Person Re-Identification</a></h5><span class="papers-author-page"><p>Guan-An Wang, Tianzhu Zhang, Yang Yang, Jian Cheng, Jianlong Chang, Xu Liang, Zeng-Guang Hou</p><p>12144-12151</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6894/6894-13-10123-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12152-context-modulated-dynamic-networks-for-actor-and-action-video-segmentation-with-language-queries/">Context Modulated Dynamic Networks for Actor and Action Video Segmentation with Language Queries</a></h5><span class="papers-author-page"><p>Hao Wang, Cheng Deng, Fan Ma, Yi Yang</p><p>12152-12159</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6895/6895-13-10124-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12160-all-you-need-is-boundary-toward-arbitrary-shaped-text-spotting/">All You Need Is Boundary: Toward Arbitrary-Shaped Text Spotting</a></h5><span class="papers-author-page"><p>Hao Wang, Pu Lu, Hui Zhang, Mingkun Yang, Xiang Bai, Yongchao Xu, Mengchao He, Yongpan Wang, Wenyu Liu</p><p>12160-12167</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6896/6896-13-10125-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12168-temporally-grounding-language-queries-in-videos-by-contextual-boundary-aware-prediction/">Temporally Grounding Language Queries in Videos by Contextual Boundary-Aware Prediction</a></h5><span class="papers-author-page"><p>Jingwen Wang, Lin Ma, Wenhao Jiang</p><p>12168-12175</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6897/6897-13-10126-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12176-show-recall-and-tell-image-captioning-with-recall-mechanism/">Show, Recall, and Tell: Image Captioning with Recall Mechanism</a></h5><span class="papers-author-page"><p>Li Wang, Zechen Bai, Yonghua Zhang, Hongtao Lu</p><p>12176-12183</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6898/6898-13-10127-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12184-post-policy-based-switch-tracking/">POST: POlicy-Based Switch Tracking</a></h5><span class="papers-author-page"><p>Ning Wang, Wengang Zhou, Guojun Qi, Houqiang Li</p><p>12184-12191</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6899/6899-13-10128-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12192-sparsity-inducing-binarized-neural-networks/">Sparsity-Inducing Binarized Neural Networks</a></h5><span class="papers-author-page"><p>Peisong Wang, Xiangyu He, Gang Li, Tianli Zhao, Jian Cheng</p><p>12192-12199</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6900/6900-13-10129-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12200-multi-speaker-video-dialog-with-frame-level-temporal-localization/">Multi-Speaker Video Dialog with Frame-Level Temporal Localization</a></h5><span class="papers-author-page"><p>Qiang Wang, Pin Jiang, Zhiyi Guo, Yahong Han, Zhou Zhao</p><p>12200-12207</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6901/6901-13-10130-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12208-rdsnet-a-new-deep-architecture-forreciprocal-object-detection-and-instance-segmentation/">RDSNet: A New Deep Architecture forReciprocal Object Detection and Instance Segmentation</a></h5><span class="papers-author-page"><p>Shaoru Wang, Yongchao Gong, Junliang Xing, Lichao Huang, Chang Huang, Weiming Hu</p><p>12208-12215</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6902/6902-13-10131-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12216-decoupled-attention-network-for-text-recognition/">Decoupled Attention Network for Text Recognition</a></h5><span class="papers-author-page"><p>Tianwei Wang, Yuanzhi Zhu, Lianwen Jin, Canjie Luo, Xiaoxue Chen, Yaqiang Wu, Qianying Wang, Mingxiang Cai</p><p>12216-12224</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6903/6903-13-10132-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12225-one-shot-learning-for-long-tail-visual-relation-detection/">One-Shot Learning for Long-Tail Visual Relation Detection</a></h5><span class="papers-author-page"><p>Weitao Wang, Meng Wang, Sen Wang, Guodong Long, Lina Yao, Guilin Qi, Yang Chen</p><p>12225-12232</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6904/6904-13-10133-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11908-ffa-net-feature-fusion-attention-network-for-single-image-dehazing/">FFA-Net: Feature Fusion Attention Network for Single Image Dehazing</a></h5><span class="papers-author-page"><p>Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, Huizhu Jia</p><p>11908-11915</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6865/6865-13-10094-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11916-learning-meta-model-for-zero-and-few-shot-face-anti-spoofing/">Learning Meta Model for Zero- and Few-Shot Face Anti-Spoofing</a></h5><span class="papers-author-page"><p>Yunxiao Qin, Chenxu Zhao, Xiangyu Zhu, Zezheng Wang, Zitong Yu, Tianyu Fu, Feng Zhou, Jingping Shi, Zhen Lei</p><p>11916-11923</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6866/6866-13-10095-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11924-dgcn-dynamic-graph-convolutional-network-for-efficient-multi-person-pose-estimation/">DGCN: Dynamic Graph Convolutional Network for Efficient Multi-Person Pose Estimation</a></h5><span class="papers-author-page"><p>Zhongwei Qiu, Kai Qiu, Jianlong Fu, Dongmei Fu</p><p>11924-11931</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6867/6867-13-10096-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11932-improved-visual-semantic-alignment-for-zero-shot-object-detection/">Improved Visual-Semantic Alignment for Zero-Shot Object Detection</a></h5><span class="papers-author-page"><p>Shafin Rahman, Salman Khan, Nick Barnes</p><p>11932-11939</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6868/6868-13-10097-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11940-dynamic-graph-representation-for-occlusion-handling-in-biometrics/">Dynamic Graph Representation for Occlusion Handling in Biometrics</a></h5><span class="papers-author-page"><p>Min Ren, Yunlong Wang, Zhenan Sun, Tieniu Tan</p><p>11940-11947</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6869/6869-13-10098-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11948-conquering-the-cnn-over-parameterization-dilemma-a-volterra-filtering-approach-for-action-recognition/">Conquering the CNN Over-Parameterization Dilemma: A Volterra Filtering Approach for Action Recognition</a></h5><span class="papers-author-page"><p>Siddharth Roheda, Hamid Krim</p><p>11948-11956</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6870/6870-13-10099-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11957-hidden-trigger-backdoor-attacks/">Hidden Trigger Backdoor Attacks</a></h5><span class="papers-author-page"><p>Aniruddha Saha, Akshayvarun Subramanya, Hamed Pirsiavash</p><p>11957-11965</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6871/6871-13-10100-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11966-temporal-interlacing-network/">Temporal Interlacing Network</a></h5><span class="papers-author-page"><p>Hao Shao, Shengju Qian, Yu Liu</p><p>11966-11973</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6872/6872-13-10101-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11974-regularized-fine-grained-meta-face-anti-spoofing/">Regularized Fine-Grained Meta Face Anti-Spoofing</a></h5><span class="papers-author-page"><p>Rui Shao, Xiangyuan Lan, Pong C. Yuen</p><p>11974-11981</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6873/6873-13-10102-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11982-multimodal-interaction-aware-trajectory-prediction-in-crowded-space/">Multimodal Interaction-Aware Trajectory Prediction in Crowded Space</a></h5><span class="papers-author-page"><p>Xiaodan Shi, Xiaowei Shao, Zipei Fan, Renhe Jiang, Haoran Zhang, Zhiling Guo, Guangming Wu, Wei Yuan, Ryosuke Shibasaki</p><p>11982-11989</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6874/6874-13-10103-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11990-optimal-feature-transport-for-cross-view-image-geo-localization/">Optimal Feature Transport for Cross-View Image Geo-Localization</a></h5><span class="papers-author-page"><p>Yujiao Shi, Xin Yu, Liu Liu, Tong Zhang, Hongdong Li</p><p>11990-11997</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6875/6875-13-10104-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11998-identifying-model-weakness-with-adversarial-examiner/">Identifying Model Weakness with Adversarial Examiner</a></h5><span class="papers-author-page"><p>Michelle Shu, Chenxi Liu, Weichao Qiu, Alan Yuille</p><p>11998-12006</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6876/6876-13-10105-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12007-efficient-residual-dense-block-search-for-image-super-resolution/">Efficient Residual Dense Block Search for Image Super-Resolution</a></h5><span class="papers-author-page"><p>Dehua Song, Chang Xu, Xu Jia, Yiyi Chen, Chunjing Xu, Yunhe Wang</p><p>12007-12014</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6877/6877-13-10106-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12015-kpnet-towards-minimal-face-detector/">KPNet: Towards Minimal Face Detector</a></h5><span class="papers-author-page"><p>Guanglu Song, Yu Liu, Yuhang Zang, Xiaogang Wang, Biao Leng, Qingsheng Yuan</p><p>12015-12022</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6878/6878-13-10107-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12023-multi-spectral-salient-object-detection-by-adversarial-domain-adaptation/">Multi-Spectral Salient Object Detection by Adversarial Domain Adaptation</a></h5><span class="papers-author-page"><p>Shaoyue Song, Hongkai Yu, Zhenjiang Miao, Jianwu Fang, Kang Zheng, Cong Ma, Song Wang</p><p>12023-12030</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6879/6879-13-10108-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12031-stereoscopic-image-super-resolution-with-stereo-consistent-feature/">Stereoscopic Image Super-Resolution with Stereo Consistent Feature</a></h5><span class="papers-author-page"><p>Wonil Song, Sungil Choi, Somi Jeong, Kwanghoon Sohn</p><p>12031-12038</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6880/6880-13-10109-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12039-an-efficient-framework-for-dense-video-captioning/">An Efficient Framework for Dense Video Captioning</a></h5><span class="papers-author-page"><p>Maitreya Suin, A. N. Rajagopalan</p><p>12039-12046</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6881/6881-13-10110-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12047-fine-grained-recognition-accounting-for-subtle-differences-between-similar-classes/">Fine-Grained Recognition: Accounting for Subtle Differences between Similar Classes</a></h5><span class="papers-author-page"><p>Guolei Sun, Hisham Cholakkal, Salman Khan, Fahad Khan, Ling Shao</p><p>12047-12054</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6882/6882-13-10111-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12055-relation-aware-pedestrian-attribute-recognition-with-graph-convolutional-networks/">Relation-Aware Pedestrian Attribute Recognition with Graph Convolutional Networks</a></h5><span class="papers-author-page"><p>Zichang Tan, Yang Yang, Jun Wan, Guodong Guo, Stan Z. Li</p><p>12055-12062</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6883/6883-13-10112-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/12063-r%c2%b2mrf-defocus-blur-detection-via-recurrently-refining-multi-scale-residual-features/">R²MRF: Defocus Blur Detection via Recurrently Refining Multi-Scale Residual Features</a></h5><span class="papers-author-page"><p>Chang Tang, Xinwang Liu, Xinzhong Zhu, En Zhu, Kun Sun, Pichao Wang, Lizhe Wang, Albert Zomaya</p><p>12063-12070</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6884/6884-13-10113-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11741-fine-grained-fashion-similarity-learning-by-attribute-specific-embedding-network/">Fine-Grained Fashion Similarity Learning by Attribute-Specific Embedding Network</a></h5><span class="papers-author-page"><p>Zhe Ma, Jianfeng Dong, Zhongzi Long, Yao Zhang, Yuan He, Hui Xue, Shouling Ji</p><p>11741-11748</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6845/6845-13-10074-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11749-domain-generalization-using-a-mixture-of-multiple-latent-domains/">Domain Generalization Using a Mixture of Multiple Latent Domains</a></h5><span class="papers-author-page"><p>Toshihiko Matsuura, Tatsuya Harada</p><p>11749-11756</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6846/6846-13-10075-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11757-high-order-residual-network-for-light-field-super-resolution/">High-Order Residual Network for Light Field Super-Resolution</a></h5><span class="papers-author-page"><p>Nan Meng, Xiaofei Wu, Jianzhuang Liu, Edmund Lam</p><p>11757-11764</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6847/6847-13-10076-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11765-shallow-feature-based-dense-attention-network-for-crowd-counting/">Shallow Feature Based Dense Attention Network for Crowd Counting</a></h5><span class="papers-author-page"><p>Yunqi Miao, Zijia Lin, Guiguang Ding, Jungong Han</p><p>11765-11772</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6848/6848-13-10077-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11773-learning-to-follow-directions-in-street-view/">Learning to Follow Directions in Street View</a></h5><span class="papers-author-page"><p>Karl Moritz Hermann, Mateusz Malinowski, Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, Raia Hadsell</p><p>11773-11781</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6849/6849-13-10078-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11782-pyramid-attention-aggregation-network-for-semantic-segmentation-of-surgical-instruments/">Pyramid Attention Aggregation Network for Semantic Segmentation of Surgical Instruments</a></h5><span class="papers-author-page"><p>Zhen-Liang Ni, Gui-Bin Bian, Guan-An Wang, Xiao-Hu Zhou, Zeng-Guang Hou, Hua-Bin Chen, Xiao-Liang Xie</p><p>11782-11790</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6850/6850-13-10079-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11791-spatial-temporal-gaussian-scale-mixture-modeling-for-foreground-estimation/">Spatial-Temporal Gaussian Scale Mixture Modeling for Foreground Estimation</a></h5><span class="papers-author-page"><p>Qian Ning, Weisheng Dong, Fangfang Wu, Jinjian Wu, Jie Lin, Guangming Shi</p><p>11791-11798</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6851/6851-13-10080-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11799-crowd-counting-with-decomposed-uncertainty/">Crowd Counting with Decomposed Uncertainty</a></h5><span class="papers-author-page"><p>Min-hwan Oh, Peder Olsen, Karthikeyan Natesan Ramamurthy</p><p>11799-11806</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6852/6852-13-10081-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11807-image-formation-model-guided-deep-image-super-resolution/">Image Formation Model Guided Deep Image Super-Resolution</a></h5><span class="papers-author-page"><p>Jinshan Pan, Yang Liu, Deqing Sun, Jimmy Ren, Ming-Ming Cheng, Jian Yang, Jinhui Tang</p><p>11807-11814</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6853/6853-13-10082-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11815-adversarial-cross-domain-action-recognition-with-co-attention/">Adversarial Cross-Domain Action Recognition with Co-Attention</a></h5><span class="papers-author-page"><p>Boxiao Pan, Zhangjie Cao, Ehsan Adeli, Juan Carlos Niebles</p><p>11815-11822</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6854/6854-13-10083-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11823-further-understanding-videos-through-adverbs-a-new-video-task/">Further Understanding Videos through Adverbs: A New Video Task</a></h5><span class="papers-author-page"><p>Bo Pang, Kaiwen Zha, Yifan Zhang, Cewu Lu</p><p>11823-11830</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6855/6855-13-10084-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11831-visual-dialogue-state-tracking-for-question-generation/">Visual Dialogue State Tracking for Question Generation</a></h5><span class="papers-author-page"><p>Wei Pang, Xiaojie Wang</p><p>11831-11838</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6856/6856-13-10085-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11839-relation-network-for-person-re-identification/">Relation Network for Person Re-Identification</a></h5><span class="papers-author-page"><p>Hyunjong Park, Bumsub Ham</p><p>11839-11847</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6857/6857-13-10086-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11848-explanation-vs-attention-a-two-player-game-to-obtain-attention-for-vqa/">Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA</a></h5><span class="papers-author-page"><p>Badri Patro, Anupriy, Vinay Namboodiri</p><p>11848-11855</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6858/6858-13-10087-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11856-lcd-learned-cross-domain-descriptors-for-2d-3d-matching/">LCD: Learned Cross-Domain Descriptors for 2D-3D Matching</a></h5><span class="papers-author-page"><p>Quang-Hieu Pham, Mikaela Angelina Uy, Binh-Son Hua, Duc Thanh Nguyen, Gemma Roig, Sai-Kit Yeung</p><p>11856-11864</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6859/6859-13-10088-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11865-exploit-and-replace-an-asymmetrical-two-stream-architecture-for-versatile-light-field-saliency-detection/">Exploit and Replace: An Asymmetrical Two-Stream Architecture for Versatile Light Field Saliency Detection</a></h5><span class="papers-author-page"><p>Yongri Piao, Zhengkun Rong, Miao Zhang, Huchuan Lu</p><p>11865-11873</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6860/6860-13-10089-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11874-differentiable-grammars-for-videos/">Differentiable Grammars for Videos</a></h5><span class="papers-author-page"><p>AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo</p><p>11874-11881</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6861/6861-13-10090-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11882-region-adaptive-dense-network-for-efficient-motion-deblurring/">Region-Adaptive Dense Network for Efficient Motion Deblurring</a></h5><span class="papers-author-page"><p>Kuldeep Purohit, A. N. Rajagopalan</p><p>11882-11889</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6862/6862-13-10091-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11890-visualizing-deep-networks-by-optimizing-with-integrated-gradients/">Visualizing Deep Networks by Optimizing with Integrated Gradients</a></h5><span class="papers-author-page"><p>Zhongang Qi, Saeed Khorram, Li Fuxin</p><p>11890-11898</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6863/6863-13-10092-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11899-text-perceptron-towards-end-to-end-arbitrary-shaped-text-spotting/">Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting</a></h5><span class="papers-author-page"><p>Liang Qiao, Sanli Tang, Zhanzhan Cheng, Yunlu Xu, Yi Niu, Shiliang Pu, Fei Wu</p><p>11899-11907</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6864/6864-13-10093-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11580-learned-video-compression-via-joint-spatial-temporal-correlation-exploration/">Learned Video Compression via Joint Spatial-Temporal Correlation Exploration</a></h5><span class="papers-author-page"><p>Haojie Liu, Han Shen, Lichao Huang, Ming Lu, Tong Chen, Zhan Ma</p><p>11580-11587</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6825/6825-13-10054-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11588-interactive-dual-generative-adversarial-networks-for-image-captioning/">Interactive Dual Generative Adversarial Networks for Image Captioning</a></h5><span class="papers-author-page"><p>Junhao Liu, Kai Wang, Chunpu Xu, Zhou Zhao, Ruifeng Xu, Ying Shen, Min Yang</p><p>11588-11595</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6826/6826-13-10055-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11596-morphing-and-sampling-network-for-dense-point-cloud-completion/">Morphing and Sampling Network for Dense Point Cloud Completion</a></h5><span class="papers-author-page"><p>Minghua Liu, Lu Sheng, Sheng Yang, Jing Shao, Shi-Min Hu</p><p>11596-11603</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6827/6827-13-10056-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11604-multi-task-driven-feature-models-for-thermal-infrared-tracking/">Multi-Task Driven Feature Models for Thermal Infrared Tracking</a></h5><span class="papers-author-page"><p>Qiao Liu, Xin Li, Zhenyu He, Nana Fan, Di Yuan, Wei Liu, Yongsheng Liang</p><p>11604-11611</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6828/6828-13-10057-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11612-progressive-boundary-refinement-network-for-temporal-action-detection/">Progressive Boundary Refinement Network for Temporal Action Detection</a></h5><span class="papers-author-page"><p>Qinying Liu, Zilei Wang</p><p>11612-11619</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6829/6829-13-10058-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11620-a-generalized-framework-for-edge-preserving-and-structure-preserving-image-smoothing/">A Generalized Framework for Edge-Preserving and Structure-Preserving Image Smoothing</a></h5><span class="papers-author-page"><p>Wei Liu, Pingping Zhang, Yinjie Lei, Xiaolin Huang, Jie Yang, Ian Reid</p><p>11620-11628</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6830/6830-13-10059-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11629-importance-aware-semantic-segmentation-in-self-driving-with-discrete-wasserstein-training/">Importance-Aware Semantic Segmentation in Self-Driving with Discrete Wasserstein Training</a></h5><span class="papers-author-page"><p>Xiaofeng Liu, Yuzhuo Han, Song Bai, Yi Ge, Tianxing Wang, Xu Han, Site Li, Jane You, Jun Lu</p><p>11629-11636</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6831/6831-13-10060-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11637-a-new-dataset-and-boundary-attention-semantic-segmentation-for-face-parsing/">A New Dataset and Boundary-Attention Semantic Segmentation for Face Parsing</a></h5><span class="papers-author-page"><p>Yinglu Liu, Hailin Shi, Hao Shen, Yue Si, Xiaobo Wang, Tao Mei</p><p>11637-11644</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6832/6832-13-10061-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11645-learning-cross-modal-context-graph-for-visual-grounding/">Learning Cross-Modal Context Graph for Visual Grounding</a></h5><span class="papers-author-page"><p>Yongfei Liu, Bo Wan, Xiaodan Zhu, Xuming He</p><p>11645-11652</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6833/6833-13-10062-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11653-cbnet-a-novel-composite-backbone-network-architecture-for-object-detection/">CBNet: A Novel Composite Backbone Network Architecture for Object Detection</a></h5><span class="papers-author-page"><p>Yudong Liu, Yongtao Wang, Siwei Wang, Tingting Liang, Qijie Zhao, Zhi Tang, Haibin Ling</p><p>11653-11660</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6834/6834-13-10063-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11661-separate-in-latent-space-unsupervised-single-image-layer-separation/">Separate in Latent Space: Unsupervised Single Image Layer Separation</a></h5><span class="papers-author-page"><p>Yunfei Liu, Feng Lu</p><p>11661-11668</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6835/6835-13-10064-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11669-teinet-towards-an-efficient-architecture-for-video-recognition/">TEINet: Towards an Efficient Architecture for Video Recognition</a></h5><span class="papers-author-page"><p>Zhaoyang Liu, Donghao Luo, Yabiao Wang, Limin Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Tong Lu</p><p>11669-11676</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6836/6836-13-10065-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11677-tanet-robust-3d-object-detection-from-point-clouds-with-triple-attention/">TANet: Robust 3D Object Detection from Point Clouds with Triple Attention</a></h5><span class="papers-author-page"><p>Zhe Liu, Xin Zhao, Tengteng Huang, Ruolan Hu, Yu Zhou, Xiang Bai</p><p>11677-11684</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6837/6837-13-10066-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11685-training-time-friendly-network-for-real-time-object-detection/">Training-Time-Friendly Network for Real-Time Object Detection</a></h5><span class="papers-author-page"><p>Zili Liu, Tu Zheng, Guodong Xu, Zheng Yang, Haifeng Liu, Deng Cai</p><p>11685-11692</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6838/6838-13-10067-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11693-hybrid-graph-neural-networks-for-crowd-counting/">Hybrid Graph Neural Networks for Crowd Counting</a></h5><span class="papers-author-page"><p>Ao Luo, Fan Yang, Xin Li, Dong Nie, Zhicheng Jiao, Shangchen Zhou, Hong Cheng</p><p>11693-11700</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6839/6839-13-10068-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11701-video-cloze-procedure-for-self-supervised-spatio-temporal-learning/">Video Cloze Procedure for Self-Supervised Spatio-Temporal Learning</a></h5><span class="papers-author-page"><p>Dezhao Luo, Chang Liu, Yu Zhou, Dongbao Yang, Can Ma, Qixiang Ye, Weiping Wang</p><p>11701-11708</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6840/6840-13-10069-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11709-context-aware-zero-shot-recognition/">Context-Aware Zero-Shot Recognition</a></h5><span class="papers-author-page"><p>Ruotian Luo, Ning Zhang, Bohyung Han, Linjie Yang</p><p>11709-11716</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6841/6841-13-10070-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11717-learning-saliency-free-model-with-generic-features-for-weakly-supervised-semantic-segmentation/">Learning Saliency-Free Model with Generic Features for Weakly-Supervised Semantic Segmentation</a></h5><span class="papers-author-page"><p>Wenfeng Luo, Meng Yang</p><p>11717-11724</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6842/6842-13-10071-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11725-an-integrated-enhancement-solution-for-24-hour-colorful-imaging/">An Integrated Enhancement Solution for 24-Hour Colorful Imaging</a></h5><span class="papers-author-page"><p>Feifan Lv, Yinqiang Zheng, Yicheng Li, Feng Lu</p><p>11725-11732</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6843/6843-13-10072-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11733-a-variational-autoencoder-with-deep-embedding-model-for-generalized-zero-shot-learning/">A Variational Autoencoder with Deep Embedding Model for Generalized Zero-Shot Learning</a></h5><span class="papers-author-page"><p>Peirong Ma, Xiao Hu</p><p>11733-11740</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6844/6844-13-10073-1-10-20200525.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11418-gated-fully-fusion-for-semantic-segmentation/">Gated Fully Fusion for Semantic Segmentation</a></h5><span class="papers-author-page"><p>Xiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, Shaohua Tan, Kuiyuan Yang</p><p>11418-11425</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6805/6805-13-10034-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11426-scalenet-improve-cnns-through-recursively-rescaling-objects/">ScaleNet &#8211; Improve CNNs through Recursively Rescaling Objects</a></h5><span class="papers-author-page"><p>Xingyi Li, Zhongang Qi, Xiaoli Fern, Fuxin Li</p><p>11426-11433</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6806/6806-13-10035-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11434-relation-guided-spatial-attention-and-temporal-refinement-for-video-based-person-re-identification/">Relation-Guided Spatial Attention and Temporal Refinement for Video-Based Person Re-Identification</a></h5><span class="papers-author-page"><p>Xingze Li, Wengang Zhou, Yun Zhou, Houqiang Li</p><p>11434-11441</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6807/6807-13-10036-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11442-geometry-driven-self-supervised-method-for-3d-human-pose-estimation/">Geometry-Driven Self-Supervised Method for 3D Human Pose Estimation</a></h5><span class="papers-author-page"><p>Yang Li, Kan Li, Shuai Jiang, Ziyue Zhang, Congzhentao Huang, Richard Yi Da Xu</p><p>11442-11449</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6808/6808-13-10037-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11450-natural-image-matting-via-guided-contextual-attention/">Natural Image Matting via Guided Contextual Attention</a></h5><span class="papers-author-page"><p>Yaoyi Li, Hongtao Lu</p><p>11450-11457</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6809/6809-13-10038-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11458-learning-transferable-adversarial-examples-via-ghost-networks/">Learning Transferable Adversarial Examples via Ghost Networks</a></h5><span class="papers-author-page"><p>Yingwei Li, Song Bai, Yuyin Zhou, Cihang Xie, Zhishuai Zhang, Alan Yuille</p><p>11458-11465</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6810/6810-13-10039-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11466-finding-action-tubes-with-a-sparse-to-dense-framework/">Finding Action Tubes with a Sparse-to-Dense Framework</a></h5><span class="papers-author-page"><p>Yuxi Li, Weiyao Lin, Tao Wang, John See, Rui Qian, Ning Xu, Limin Wang, Shugong Xu</p><p>11466-11473</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6811/6811-13-10040-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11474-real-time-scene-text-detection-with-differentiable-binarization/">Real-Time Scene Text Detection with Differentiable Binarization</a></h5><span class="papers-author-page"><p>Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, Xiang Bai</p><p>11474-11481</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6812/6812-13-10041-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11482-object-instance-mining-for-weakly-supervised-object-detection/">Object Instance Mining for Weakly Supervised Object Detection</a></h5><span class="papers-author-page"><p>Chenhao Lin, Siwen Wang, Dongqi Xu, Yu Lu, Wayne Zhang</p><p>11482-11489</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6813/6813-13-10042-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11490-multimodal-structure-consistent-image-to-image-translation/">Multimodal Structure-Consistent Image-to-Image Translation</a></h5><span class="papers-author-page"><p>Che-Tsung Lin, Yen-Yi Wu, Po-Hao Hsu, Shang-Hong Lai</p><p>11490-11498</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6814/6814-13-10043-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11499-fast-learning-of-temporal-action-proposal-via-dense-boundary-generator/">Fast Learning of Temporal Action Proposal via Dense Boundary Generator</a></h5><span class="papers-author-page"><p>Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao Luo, Zhipeng Cui, Chengjie Wang, Jilin Li, Feiyue Huang, Rongrong Ji</p><p>11499-11506</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6815/6815-13-10044-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11507-learning-to-transfer-unsupervised-domain-translation-via-meta-learning/">Learning to Transfer: Unsupervised Domain Translation via Meta-Learning</a></h5><span class="papers-author-page"><p>Jianxin Lin, Yijun Wang, Zhibo Chen, Tianyu He</p><p>11507-11514</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6816/6816-13-10045-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11515-learning-cross-aligned-latent-embeddings-for-zero-shot-cross-modal-retrieval/">Learning Cross-Aligned Latent Embeddings for Zero-Shot Cross-Modal Retrieval</a></h5><span class="papers-author-page"><p>Kaiyi Lin, Xing Xu, Lianli Gao, Zheng Wang, Heng Tao Shen</p><p>11515-11522</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6817/6817-13-10046-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11523-learning-to-deblur-face-images-via-sketch-synthesis/">Learning to Deblur Face Images via Sketch Synthesis</a></h5><span class="papers-author-page"><p>Songnan Lin, Jiawei Zhang, Jinshan Pan, Yicun Liu, Yongtian Wang, Jing Chen, Jimmy Ren</p><p>11523-11530</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6818/6818-13-10047-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11531-self-attention-convlstm-for-spatiotemporal-prediction/">Self-Attention ConvLSTM for Spatiotemporal Prediction</a></h5><span class="papers-author-page"><p>Zhihui Lin, Maomao Li, Zhuobin Zheng, Yangyang Cheng, Chun Yuan</p><p>11531-11538</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6819/6819-13-10048-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11539-weakly-supervised-video-moment-retrieval-via-semantic-completion-network/">Weakly-Supervised Video Moment Retrieval via Semantic Completion Network</a></h5><span class="papers-author-page"><p>Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang, Huasheng Liu</p><p>11539-11546</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6820/6820-13-10049-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11547-zero-shot-learning-from-adversarial-feature-residual-to-compact-visual-feature/">Zero-Shot Learning from Adversarial Feature Residual to Compact Visual Feature</a></h5><span class="papers-author-page"><p>Bo Liu, Qiulei Dong, Zhanyi Hu</p><p>11547-11554</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6821/6821-13-10050-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11555-filtration-and-distillation-enhancing-region-attention-for-fine-grained-visual-categorization/">Filtration and Distillation: Enhancing Region Attention for Fine-Grained Visual Categorization</a></h5><span class="papers-author-page"><p>Chuanbin Liu, Hongtao Xie, Zheng-Jun Zha, Lingfeng Ma, Lingyun Yu, Yongdong Zhang</p><p>11555-11562</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6822/6822-13-10051-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11563-hal-improved-text-image-matching-by-mitigating-visual-semantic-hubs/">HAL: Improved Text-Image Matching by Mitigating Visual Semantic Hubs</a></h5><span class="papers-author-page"><p>Fangyu Liu, Rongtian Ye, Xun Wang, Shuaipeng Li</p><p>11563-11571</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6823/6823-13-10052-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11572-federated-learning-for-vision-and-language-grounding-problems/">Federated Learning for Vision-and-Language Grounding Problems</a></h5><span class="papers-author-page"><p>Fenglin Liu, Xian Wu, Shen Ge, Wei Fan, Yuexian Zou</p><p>11572-11579</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6824/6824-13-10053-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11254-mule-multimodal-universal-language-embedding/">MULE: Multimodal Universal Language Embedding</a></h5><span class="papers-author-page"><p>Donghyun Kim, Kuniaki Saito, Kate Saenko, Stan Sclaroff, Bryan Plummer</p><p>11254-11261</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6785/6785-13-10014-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11262-rest-performance-improvement-of-a-black-box-model-via-rl-based-spatial-transformation/">REST: Performance Improvement of a Black Box Model via RL-Based Spatial Transformation</a></h5><span class="papers-author-page"><p>Jae Myung Kim, Hyungjin Kim, Chanwoo Park, Jungwoo Lee</p><p>11262-11269</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6786/6786-13-10015-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11270-spiking-yolo-spiking-neural-network-for-energy-efficient-object-detection/">Spiking-YOLO: Spiking Neural Network for Energy-Efficient Object Detection</a></h5><span class="papers-author-page"><p>Seijoon Kim, Seongsik Park, Byunggook Na, Sungroh Yoon</p><p>11270-11277</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6787/6787-13-10016-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11278-fisr-deep-joint-frame-interpolation-and-super-resolution-with-a-multi-scale-temporal-loss/">FISR: Deep Joint Frame Interpolation and Super-Resolution with a Multi-Scale Temporal Loss</a></h5><span class="papers-author-page"><p>Soo Ye Kim, Jihyong Oh, Munchurl Kim</p><p>11278-11286</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6788/6788-13-10017-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11287-jsi-gan-gan-based-joint-super-resolution-and-inverse-tone-mapping-with-pixel-wise-task-specific-filters-for-uhd-hdr-video/">JSI-GAN: GAN-Based Joint Super-Resolution and Inverse Tone-Mapping with Pixel-Wise Task-Specific Filters for UHD HDR Video</a></h5><span class="papers-author-page"><p>Soo Ye Kim, Jihyong Oh, Munchurl Kim</p><p>11287-11295</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6789/6789-13-10018-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11296-unpaired-image-enhancement-featuring-reinforcement-learning-controlled-image-editing-software/">Unpaired Image Enhancement Featuring Reinforcement-Learning-Controlled Image Editing Software</a></h5><span class="papers-author-page"><p>Satoshi Kosugi, Toshihiko Yamasaki</p><p>11296-11303</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6790/6790-13-10019-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11304-adversary-for-social-good-protecting-familial-privacy-through-joint-adversarial-attacks/">Adversary for Social Good: Protecting Familial Privacy through Joint Adversarial Attacks</a></h5><span class="papers-author-page"><p>Chetan Kumar, Riazat Ryan, Ming Shao</p><p>11304-11311</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6791/6791-13-10020-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11312-kinematic-structure-preserved-representation-for-unsupervised-3d-human-pose-estimation/">Kinematic-Structure-Preserved Representation for Unsupervised 3D Human Pose Estimation</a></h5><span class="papers-author-page"><p>Jogendra Nath Kundu, Siddharth Seth, Rahul M V, Mugalodi Rakesh, Venkatesh Babu Radhakrishnan, Anirban Chakraborty</p><p>11312-11319</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6792/6792-13-10021-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11320-background-suppression-network-for-weakly-supervised-temporal-action-localization/">Background Suppression Network for Weakly-Supervised Temporal Action Localization</a></h5><span class="papers-author-page"><p>Pilhyeon Lee, Youngjung Uh, Hyeran Byun</p><p>11320-11327</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6793/6793-13-10022-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11328-multi-question-learning-for-visual-question-answering/">Multi-Question Learning for Visual Question Answering</a></h5><span class="papers-author-page"><p>Chenyi Lei, Lei Wu, Dong Liu, Zhao Li, Guoxin Wang, Haihong Tang, Houqiang Li</p><p>11328-11335</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6794/6794-13-10023-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11336-unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training/">Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training</a></h5><span class="papers-author-page"><p>Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang</p><p>11336-11344</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6795/6795-13-10024-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11345-multi-spectral-vehicle-re-identification-a-challenge/">Multi-Spectral Vehicle Re-Identification: A Challenge</a></h5><span class="papers-author-page"><p>Hongchao Li, Chenglong Li, Xianpeng Zhu, Aihua Zheng, Bin Luo</p><p>11345-11353</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6796/6796-13-10025-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11354-simple-pose-rethinking-and-improving-a-bottom-up-approach-for-multi-person-pose-estimation/">Simple Pose: Rethinking and Improving a Bottom-up Approach for Multi-Person Pose Estimation</a></h5><span class="papers-author-page"><p>Jia Li, Wen Su, Zengfu Wang</p><p>11354-11361</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6797/6797-13-10026-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11362-learning-part-generation-and-assembly-for-structure-aware-shape-synthesis/">Learning Part Generation and Assembly for Structure-Aware Shape Synthesis</a></h5><span class="papers-author-page"><p>Jun Li, Chengjie Niu, Kai Xu</p><p>11362-11369</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6798/6798-13-10027-1-10-20200523.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11370-hierarchical-knowledge-squeezed-adversarial-network-compression/">Hierarchical Knowledge Squeezed Adversarial Network Compression</a></h5><span class="papers-author-page"><p>Peng Li, Chang Shu, Yuan Xie, Yan Qu, Hui Kong</p><p>11370-11377</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6799/6799-13-10028-1-10-20200523.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11378-age-progression-and-regression-with-spatial-attention-modules/">Age Progression and Regression with Spatial Attention Modules</a></h5><span class="papers-author-page"><p>Qi Li, Yunfan Liu, Zhenan Sun</p><p>11378-11385</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6800/6800-13-10029-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11386-domain-conditioned-adaptation-network/">Domain Conditioned Adaptation Network</a></h5><span class="papers-author-page"><p>Shuang Li, Chi Liu, Qiuxia Lin, Binhui Xie, Zhengming Ding, Gao Huang, Jian Tang</p><p>11386-11393</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6801/6801-13-10030-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11394-appearance-and-motion-enhancement-for-video-based-person-re-identification/">Appearance and Motion Enhancement for Video-Based Person Re-Identification</a></h5><span class="papers-author-page"><p>Shuzhao Li, Huimin Yu, Haoji Hu</p><p>11394-11401</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6802/6802-13-10031-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11402-attention-based-multi-modal-fusion-network-for-semantic-scene-completion/">Attention-Based Multi-Modal Fusion Network for Semantic Scene Completion</a></h5><span class="papers-author-page"><p>Siqi Li, Changqing Zou, Yipeng Li, Xibin Zhao, Yue Gao</p><p>11402-11409</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6803/6803-13-10032-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11410-ovl-one-view-learning-for-human-retrieval/">OVL: One-View Learning for Human Retrieval</a></h5><span class="papers-author-page"><p>Wenjing Li, Zhongcheng Wu</p><p>11410-11417</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6804/6804-13-10033-1-10-20200524.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11093-elixirnet-relation-aware-network-architecture-adaptation-for-medical-lesion-detection/">ElixirNet: Relation-Aware Network Architecture Adaptation for Medical Lesion Detection</a></h5><span class="papers-author-page"><p>Chenhan Jiang, Shaoju Wang, Xiaodan Liang, Hang Xu, Nong Xiao</p><p>11093-11100</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6765/6765-13-9994-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11101-divide-and-conquer-question-guided-spatio-temporal-contextual-attention-for-video-question-answering/">Divide and Conquer: Question-Guided Spatio-Temporal Contextual Attention for Video Question Answering</a></h5><span class="papers-author-page"><p>Jianwen Jiang, Ziqiang Chen, Haojie Lin, Xibin Zhao, Yue Gao</p><p>11101-11108</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6766/6766-13-9995-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11109-reasoning-with-heterogeneous-graph-alignment-for-video-question-answering/">Reasoning with Heterogeneous Graph Alignment for Video Question Answering</a></h5><span class="papers-author-page"><p>Pin Jiang, Yahong Han</p><p>11109-11116</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6767/6767-13-9996-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11117-recurrent-nested-model-for-sequence-generation/">Recurrent Nested Model for Sequence Generation</a></h5><span class="papers-author-page"><p>Wenhao Jiang, Lin Ma, Wei Lu</p><p>11117-11124</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6768/6768-13-9997-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11125-dualvd-an-adaptive-dual-encoding-model-for-deep-visual-understanding-in-visual-dialogue/">DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue</a></h5><span class="papers-author-page"><p>Xiaoze Jiang, Jing Yu, Zengchang Qin, Yingying Zhuang, Xingxing Zhang, Yue Hu, Qi Wu</p><p>11125-11132</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6769/6769-13-9998-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11133-rethinking-temporal-fusion-for-video-based-person-re-identification-on-semantic-and-time-aspect/">Rethinking Temporal Fusion for Video-Based Person Re-Identification on Semantic and Time Aspect</a></h5><span class="papers-author-page"><p>Xinyang Jiang, Yifei Gong, Xiaowei Guo, Qize Yang, Feiyue Huang, WEI-SHI ZHENG, Feng Zheng, Xing Sun</p><p>11133-11140</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6770/6770-13-9999-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11141-learning-light-field-angular-super-resolution-via-a-geometry-aware-network/">Learning Light Field Angular Super-Resolution via a Geometry-Aware Network</a></h5><span class="papers-author-page"><p>Jing Jin, Junhui Hou, Hui Yuan, Sam Kwong</p><p>11141-11148</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6771/6771-13-10000-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11149-eac-net-efficient-and-accurate-convolutional-network-for-video-recognition/">EAC-Net: Efficient and Accurate Convolutional Network for Video Recognition</a></h5><span class="papers-author-page"><p>Bowei Jin, Zhuo Xu</p><p>11149-11156</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6772/6772-13-10001-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11157-ssah-semi-supervised-adversarial-deep-hashing-with-self-paced-hard-sample-generation/">SSAH: Semi-Supervised Adversarial Deep Hashing with Self-Paced Hard Sample Generation</a></h5><span class="papers-author-page"><p>Sheng Jin, Shangchen Zhou, Yao Liu, Chao Chen, Xiaoshuai Sun, Hongxun Yao, Xian-Sheng Hua</p><p>11157-11164</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6773/6773-13-10002-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11165-uncertainty-aware-multi-shot-knowledge-distillation-for-image-based-object-re-identification/">Uncertainty-Aware Multi-Shot Knowledge Distillation for Image-Based Object Re-Identification</a></h5><span class="papers-author-page"><p>Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen</p><p>11165-11172</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6774/6774-13-10003-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11173-semantics-aligned-representation-learning-for-person-re-identification/">Semantics-Aligned Representation Learning for Person Re-Identification</a></h5><span class="papers-author-page"><p>Xin Jin, Cuiling Lan, Wenjun Zeng, Guoqiang Wei, Zhibo Chen</p><p>11173-11180</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6775/6775-13-10004-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11181-overcoming-language-priors-in-vqa-via-decomposed-linguistic-representations/">Overcoming Language Priors in VQA via Decomposed Linguistic Representations</a></h5><span class="papers-author-page"><p>Chenchen Jing, Yuwei Wu, Xiaoxun Zhang, Yunde Jia, Qi Wu</p><p>11181-11188</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6776/6776-13-10005-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11189-pose-guided-multi-granularity-attention-network-for-text-based-person-search/">Pose-Guided Multi-Granularity Attention Network for Text-Based Person Search</a></h5><span class="papers-author-page"><p>Ya Jing, Chenyang Si, Junbo Wang, Wei Wang, Liang Wang, Tieniu Tan</p><p>11189-11196</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6777/6777-13-10006-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11197-associative-variational-auto-encoder-with-distributed-latent-spaces-and-associators/">Associative Variational Auto-Encoder with Distributed Latent Spaces and Associators</a></h5><span class="papers-author-page"><p>Dae Ung Jo, ByeongJu Lee, Jongwon Choi, Haanju Yoo, Jin Young Choi</p><p>11197-11204</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6778/6778-13-10007-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11205-real-time-object-tracking-via-meta-learning-efficient-model-adaptation-and-one-shot-channel-pruning/">Real-Time Object Tracking via Meta-Learning: Efficient Model Adaptation and One-Shot Channel Pruning</a></h5><span class="papers-author-page"><p>Ilchae Jung, Kihyun You, Hyeonwoo Noh, Minsu Cho, Bohyung Han</p><p>11205-11212</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6779/6779-13-10008-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11213-hide-and-tell-learning-to-bridge-photo-streams-for-visual-storytelling/">Hide-and-Tell: Learning to Bridge Photo Streams for Visual Storytelling</a></h5><span class="papers-author-page"><p>Yunjae Jung, Dahun Kim, Sanghyun Woo, Kyungsu Kim, Sungjin Kim, In So Kweon</p><p>11213-11220</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6780/6780-13-10009-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11221-synthetic-depth-transfer-for-monocular-3d-object-pose-estimation-in-the-wild/">Synthetic Depth Transfer for Monocular 3D Object Pose Estimation in the Wild</a></h5><span class="papers-author-page"><p>Yueying Kao, Weiming Li, Qiang Wang, Zhouchen Lin, Wooshik Kim, Sunghoon Hong</p><p>11221-11228</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6781/6781-13-10010-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11229-group-wise-dynamic-dropout-based-on-latent-semantic-variations/">Group-Wise Dynamic Dropout Based on Latent Semantic Variations</a></h5><span class="papers-author-page"><p>Zhiwei Ke, Zhiwei Wen, Weicheng Xie, Yi Wang, Linlin Shen</p><p>11229-11236</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6782/6782-13-10011-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11237-deep-generative-probabilistic-graph-neural-networks-for-scene-graph-generation/">Deep Generative Probabilistic Graph Neural Networks for Scene Graph Generation</a></h5><span class="papers-author-page"><p>Mahmoud Khademi, Oliver Schulte</p><p>11237-11245</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6783/6783-13-10012-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11246-tell-me-what-they-re-holding-weakly-supervised-object-detection-with-transferable-knowledge-from-human-object-interaction/">Tell Me What They&#8217;re Holding: Weakly-Supervised Object Detection with Transferable Knowledge from Human-Object Interaction</a></h5><span class="papers-author-page"><p>Daesik Kim, Gyujeong Lee, Jisoo Jeong, Nojun Kwak</p><p>11246-11253</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6784/6784-13-10013-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10933-tensor-fista-net-for-real-time-snapshot-compressive-imaging/">Tensor FISTA-Net for Real-Time Snapshot Compressive Imaging</a></h5><span class="papers-author-page"><p>Xiaochen Han, Bo Wu, Zheng Shou, Xiao-Yang Liu, Yimeng Zhang, Linghe Kong</p><p>10933-10940</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6726/6726-13-9955-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10941-temporal-context-enhanced-feature-aggregation-for-video-object-detection/">Temporal Context Enhanced Feature Aggregation for Video Object Detection</a></h5><span class="papers-author-page"><p>Fei He, Naiyu Gao, Qiaozhe Li, Senyao Du, Xin Zhao, Kaiqi Huang</p><p>10941-10948</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6727/6727-13-9956-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10949-grapy-ml-graph-pyramid-mutual-learning-for-cross-dataset-human-parsing/">Grapy-ML: Graph Pyramid Mutual Learning for Cross-Dataset Human Parsing</a></h5><span class="papers-author-page"><p>Haoyu He, Jing Zhang, Qiming Zhang, Dacheng Tao</p><p>10949-10956</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6728/6728-13-9957-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10957-softmax-dissection-towards-understanding-intra-and-inter-class-objective-for-embedding-learning/">Softmax Dissection: Towards Understanding Intra- and Inter-Class Objective for Embedding Learning</a></h5><span class="papers-author-page"><p>Lanqing He, Zhongdao Wang, Yali Li, Shengjin Wang</p><p>10957-10964</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6729/6729-13-9958-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10965-roadtagger-robust-road-attribute-inference-with-graph-neural-networks/">RoadTagger: Robust Road Attribute Inference with Graph Neural Networks</a></h5><span class="papers-author-page"><p>Songtao He, Favyen Bastani, Satvat Jagwani, Edward Park, Sofiane Abbar, Mohammad Alizadeh, Hari Balakrishnan, Sanjay Chawla, Samuel Madden, Mohammad Amin Sadeghi</p><p>10965-10972</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6730/6730-13-9959-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10973-joint-commonsense-and-relation-reasoning-for-image-and-video-captioning/">Joint Commonsense and Relation Reasoning for Image and Video Captioning</a></h5><span class="papers-author-page"><p>Jingyi Hou, Xinxiao Wu, Xiaoxun Zhang, Yayun Qi, Yunde Jia, Jiebo Luo</p><p>10973-10980</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6731/6731-13-9960-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10981-hierarchical-modes-exploring-in-generative-adversarial-networks/">Hierarchical Modes Exploring in Generative Adversarial Networks</a></h5><span class="papers-author-page"><p>Mengxiao Hu, Jinlong Li, Maolin Hu, Tao Hu</p><p>10981-10988</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6732/6732-13-9961-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10989-spstracker-sub-peak-suppression-of-response-map-for-robust-object-tracking/">SPSTracker: Sub-Peak Suppression of Response Map for Robust Object Tracking</a></h5><span class="papers-author-page"><p>Qintao Hu, Lijun Zhou, Xiaoxiao Wang, Yao Mao, Jianlin Zhang, Qixiang Ye</p><p>10989-10996</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6733/6733-13-9962-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10997-3d-shape-completion-with-multi-view-consistent-inference/">3D Shape Completion with Multi-View Consistent Inference</a></h5><span class="papers-author-page"><p>Tao Hu, Zhizhong Han, Matthias Zwicker</p><p>10997-11004</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6734/6734-13-9963-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11005-gtc-guided-training-of-ctc-towards-efficient-and-accurate-scene-text-recognition/">GTC: Guided Training of CTC towards Efficient and Accurate Scene Text Recognition</a></h5><span class="papers-author-page"><p>Wenyang Hu, Xiaocong Cai, Jun Hou, Shuai Yi, Zhiping Lin</p><p>11005-11012</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6735/6735-13-9964-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11013-coarse-to-fine-hyper-prior-modeling-for-learned-image-compression/">Coarse-to-Fine Hyper-Prior Modeling for Learned Image Compression</a></h5><span class="papers-author-page"><p>Yueyu Hu, Wenhan Yang, Jiaying Liu</p><p>11013-11020</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6736/6736-13-9965-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11021-location-aware-graph-convolutional-networks-for-video-question-answering/">Location-Aware Graph Convolutional Networks for Video Question Answering</a></h5><span class="papers-author-page"><p>Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, Chuang Gan</p><p>11021-11028</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6737/6737-13-9966-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11029-unsupervised-deep-learning-via-affinity-diffusion/">Unsupervised Deep Learning via Affinity Diffusion</a></h5><span class="papers-author-page"><p>Jiabo Huang, Qi Dong, Shaogang Gong, Xiatian Zhu</p><p>11029-11036</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6757/6757-13-9986-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11037-globaltrack-a-simple-and-strong-baseline-for-long-term-tracking/">GlobalTrack: A Simple and Strong Baseline for Long-Term Tracking</a></h5><span class="papers-author-page"><p>Lianghua Huang, Xin Zhao, Kaiqi Huang</p><p>11037-11044</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6758/6758-13-9987-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11045-part-level-graph-convolutional-network-for-skeleton-based-action-recognition/">Part-Level Graph Convolutional Network for Skeleton-Based Action Recognition</a></h5><span class="papers-author-page"><p>Linjiang Huang, Yan Huang, Wanli Ouyang, Liang Wang</p><p>11045-11052</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6759/6759-13-9988-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11053-relational-prototypical-network-for-weakly-supervised-temporal-action-localization/">Relational Prototypical Network for Weakly Supervised Temporal Action Localization</a></h5><span class="papers-author-page"><p>Linjiang Huang, Yan Huang, Wanli Ouyang, Liang Wang</p><p>11053-11060</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6760/6760-13-9989-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11061-awr-adaptive-weighting-regression-for-3d-hand-pose-estimation/">AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation</a></h5><span class="papers-author-page"><p>Weiting Huang, Pengfei Ren, Jingyu Wang, Qi Qi, Haifeng Sun</p><p>11061-11068</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6761/6761-13-9990-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11069-domain-adaptive-attention-learning-for-unsupervised-person-re-identification/">Domain Adaptive Attention Learning for Unsupervised Person Re-Identification</a></h5><span class="papers-author-page"><p>Yangru Huang, Peixi Peng, Yi Jin, Yidong Li, Junliang Xing</p><p>11069-11076</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6762/6762-13-9991-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11077-weakly-supervised-video-re-localization-with-multiscale-attention-model/">Weakly-Supervised Video Re-Localization with Multiscale Attention Model</a></h5><span class="papers-author-page"><p>Yung-Han Huang, Kuang-Jui Hsu, Shyh-Kang Jeng, Yen-Yu Lin</p><p>11077-11084</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6763/6763-13-9992-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/11085-sgap-net-semantic-guided-attentive-prototypes-network-for-few-shot-human-object-interaction-recognition/">SGAP-Net: Semantic-Guided Attentive Prototypes Network for Few-Shot Human-Object Interaction Recognition</a></h5><span class="papers-author-page"><p>Zhong Ji, Xiyao Liu, Yanwei Pang, Xuelong Li</p><p>11085-11092</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6764/6764-13-9993-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10770-scale-wise-convolution-for-image-restoration/">Scale-Wise Convolution for Image Restoration</a></h5><span class="papers-author-page"><p>Yuchen Fan, Jiahui Yu, Ding Liu, Thomas S. Huang</p><p>10770-10777</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6706/6706-13-9935-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10778-ehsod-cam-guided-end-to-end-hybrid-supervised-object-detection-with-cascade-refinement/">EHSOD: CAM-Guided End-to-End Hybrid-Supervised Object Detection with Cascade Refinement</a></h5><span class="papers-author-page"><p>Linpu Fang, Hang Xu, Zhili Liu, Sarah Parisot, Zhenguo Li</p><p>10778-10785</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6707/6707-13-9936-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10786-adversarial-attack-on-deep-product-quantization-network-for-image-retrieval/">Adversarial Attack on Deep Product Quantization Network for Image Retrieval</a></h5><span class="papers-author-page"><p>Yan Feng, Bin Chen, Tao Dai, Shu-Tao Xia</p><p>10786-10793</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6708/6708-13-9937-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10794-dynamic-sampling-network-for-semantic-segmentation/">Dynamic Sampling Network for Semantic Segmentation</a></h5><span class="papers-author-page"><p>Bin Fu, Junjun He, Zhengfu Zhang, Yu Qiao</p><p>10794-10801</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6709/6709-13-9938-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10802-ultrafast-video-attention-prediction-with-coupled-knowledge-distillation/">Ultrafast Video Attention Prediction with Coupled Knowledge Distillation</a></h5><span class="papers-author-page"><p>Kui Fu, Peipei Shi, Yafei Song, Shiming Ge, Xiangju Lu, Jia Li</p><p>10802-10809</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6710/6710-13-9939-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10810-accurate-temporal-action-proposal-generation-with-relation-aware-pyramid-network/">Accurate Temporal Action Proposal Generation with Relation-Aware Pyramid Network</a></h5><span class="papers-author-page"><p>Jialin Gao, Zhixiang Shi, Guanshuo Wang, Jiani Li, Yufeng Yuan, Shiming Ge, Xi Zhou</p><p>10810-10817</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6711/6711-13-9940-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10818-channel-interaction-networks-for-fine-grained-image-categorization/">Channel Interaction Networks for Fine-Grained Image Categorization</a></h5><span class="papers-author-page"><p>Yu Gao, Xintong Han, Xun Wang, Weilin Huang, Matthew Scott</p><p>10818-10825</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6712/6712-13-9941-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10826-knowit-vqa-answering-knowledge-based-questions-about-videos/">KnowIT VQA: Answering Knowledge-Based Questions about Videos</a></h5><span class="papers-author-page"><p>Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima</p><p>10826-10834</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6713/6713-13-9942-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10835-deep-reinforcement-learning-for-active-human-pose-estimation/">Deep Reinforcement Learning for Active Human Pose Estimation</a></h5><span class="papers-author-page"><p>Erik Gärtner, Aleksis Pirinen, Cristian Sminchisescu</p><p>10835-10844</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6714/6714-13-9943-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10845-look-one-and-more-distilling-hybrid-order-relational-knowledge-for-cross-resolution-image-recognition/">Look One and More: Distilling Hybrid Order Relational Knowledge for Cross-Resolution Image Recognition</a></h5><span class="papers-author-page"><p>Shiming Ge, Kangkai Zhang, Haolin Liu, Yingying Hua, Shengwei Zhao, Xin Jin, Hao Wen</p><p>10845-10852</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6715/6715-13-9944-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10853-symmetrical-synthesis-for-deep-metric-learning/">Symmetrical Synthesis for Deep Metric Learning</a></h5><span class="papers-author-page"><p>Geonmo Gu, Byungsoo Ko</p><p>10853-10860</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6716/6716-13-9945-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10861-flnet-landmark-driven-fetching-and-learning-network-for-faithful-talking-facial-animation-synthesis/">FLNet: Landmark Driven Fetching and Learning Network for Faithful Talking Facial Animation Synthesis</a></h5><span class="papers-author-page"><p>Kuangxiao Gu, Yuqian Zhou, Thomas Huang</p><p>10861-10868</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6717/6717-13-9946-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10869-pyramid-constrained-self-attention-network-for-fast-video-salient-object-detection/">Pyramid Constrained Self-Attention Network for Fast Video Salient Object Detection</a></h5><span class="papers-author-page"><p>Yuchao Gu, Lijuan Wang, Ziqin Wang, Yun Liu, Ming-Ming Cheng, Shao-Ping Lu</p><p>10869-10876</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6718/6718-13-9947-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10877-constructing-multiple-tasks-for-augmentation-improving-neural-image-classification-with-k-means-features/">Constructing Multiple Tasks for Augmentation: Improving Neural Image Classification with K-Means Features</a></h5><span class="papers-author-page"><p>Tao Gui, Lizhi Qing, Qi Zhang, Jiacheng Ye, Hang Yan, Zichu Fei, Xuanjing Huang</p><p>10877-10884</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6719/6719-13-9948-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10885-channel-pruning-guided-by-classification-loss-and-feature-importance/">Channel Pruning Guided by Classification Loss and Feature Importance</a></h5><span class="papers-author-page"><p>Jinyang Guo, Wanli Ouyang, Dong Xu</p><p>10885-10892</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6720/6720-13-9949-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10893-marionette-few-shot-face-reenactment-preserving-identity-of-unseen-targets/">MarioNETte: Few-Shot Face Reenactment Preserving Identity of Unseen Targets</a></h5><span class="papers-author-page"><p>Sungjoo Ha, Martin Kersner, Beomsu Kim, Seokjun Seo, Dongyoung Kim</p><p>10893-10900</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6721/6721-13-9950-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10901-sada-semantic-adversarial-diagnostic-attacks-for-autonomous-applications/">SADA: Semantic Adversarial Diagnostic Attacks for Autonomous Applications</a></h5><span class="papers-author-page"><p>Abdullah Hamdi, Matthias Mueller, Bernard Ghanem</p><p>10901-10908</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6722/6722-13-9951-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10909-robust-conditional-gan-from-uncertainty-aware-pairwise-comparisons/">Robust Conditional GAN from Uncertainty-Aware Pairwise Comparisons</a></h5><span class="papers-author-page"><p>Ligong Han, Ruijiang Gao, Mun Kim, Xin Tao, Bo Liu, Dimitris Metaxas</p><p>10909-10916</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6723/6723-13-9952-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10917-complementary-view-multiple-human-tracking/">Complementary-View Multiple Human Tracking</a></h5><span class="papers-author-page"><p>Ruize Han, Wei Feng, Jiewen Zhao, Zicheng Niu, Yujun Zhang, Liang Wan, Song Wang</p><p>10917-10924</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6724/6724-13-9953-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10925-point2node-correlation-learning-of-dynamic-node-for-point-cloud-feature-modeling/">Point2Node: Correlation Learning of Dynamic-Node for Point Cloud Feature Modeling</a></h5><span class="papers-author-page"><p>Wenkai Han, Chenglu Wen, Cheng Wang, Xin Li, Qing Li</p><p>10925-10932</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6725/6725-13-9954-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10607-video-frame-interpolation-via-deformable-separable-convolution/">Video Frame Interpolation via Deformable Separable Convolution</a></h5><span class="papers-author-page"><p>Xianhang Cheng, Zhenzhong Chen</p><p>10607-10614</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6634/6634-13-9862-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10615-cspn-learning-context-and-resource-aware-convolutional-spatial-propagation-networks-for-depth-completion/">CSPN++: Learning Context and Resource Aware Convolutional Spatial Propagation Networks for Depth Completion</a></h5><span class="papers-author-page"><p>Xinjing Cheng, Peng Wang, Chenye Guan, Ruigang Yang</p><p>10615-10622</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6635/6635-13-9863-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10623-a-coarse-to-fine-adaptive-network-for-appearance-based-gaze-estimation/">A Coarse-to-Fine Adaptive Network for Appearance-Based Gaze Estimation</a></h5><span class="papers-author-page"><p>Yihua Cheng, Shiyao Huang, Fei Wang, Chen Qian, Feng Lu</p><p>10623-10630</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6636/6636-13-9864-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10631-3d-human-pose-estimation-using-spatio-temporal-networks-with-explicit-occlusion-training/">3D Human Pose Estimation Using Spatio-Temporal Networks with Explicit Occlusion Training</a></h5><span class="papers-author-page"><p>Yu Cheng, Bo Yang, Bo Wang, Robby T. Tan</p><p>10631-10638</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6689/6689-13-9918-1-10-20200521.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10639-pedhunter-occlusion-robust-pedestrian-detector-in-crowded-scenes/">PedHunter: Occlusion Robust Pedestrian Detector in Crowded Scenes</a></h5><span class="papers-author-page"><p>Cheng Chi, Shifeng Zhang, Junliang Xing, Zhen Lei, Stan Z. Li, Xudong Zou</p><p>10639-10646</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6690/6690-13-9919-1-10-20200521.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10647-relational-learning-for-joint-head-and-human-detection/">Relational Learning for Joint Head and Human Detection</a></h5><span class="papers-author-page"><p>Cheng Chi, Shifeng Zhang, Junliang Xing, Zhen Lei, Stan Z. Li, Xudong Zou</p><p>10647-10654</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6691/6691-13-9920-1-10-20200521.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10655-visual-domain-adaptation-by-consensus-based-transfer-to-intermediate-domain/">Visual Domain Adaptation by Consensus-Based Transfer to Intermediate Domain</a></h5><span class="papers-author-page"><p>Jongwon Choi, Youngjoon Choi, Jihoon Kim, Jinyeop Chang, Ilhwan Kwon, Youngjune Gwon, Seungjai Min</p><p>10655-10662</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6692/6692-13-9921-1-10-20200521.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10663-channel-attention-is-all-you-need-for-video-frame-interpolation/">Channel Attention Is All You Need for Video Frame Interpolation</a></h5><span class="papers-author-page"><p>Myungsub Choi, Heewon Kim, Bohyung Han, Ning Xu, Kyoung Mu Lee</p><p>10663-10671</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6693/6693-13-9922-1-10-20200521.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10672-dasot-a-unified-framework-integrating-data-association-and-single-object-tracking-for-online-multi-object-tracking/">DASOT: A Unified Framework Integrating Data Association and Single Object Tracking for Online Multi-Object Tracking</a></h5><span class="papers-author-page"><p>Qi Chu, Wanli Ouyang, Bin Liu, Feng Zhu, Nenghai Yu</p><p>10672-10679</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6694/6694-13-9923-1-10-20200521.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10680-towards-ghost-free-shadow-removal-via-dual-hierarchical-aggregation-network-and-shadow-matting-gan/">Towards Ghost-Free Shadow Removal via Dual Hierarchical Aggregation Network and Shadow Matting GAN</a></h5><span class="papers-author-page"><p>Xiaodong Cun, Chi-Man Pun, Cheng Shi</p><p>10680-10687</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6695/6695-13-9924-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10688-the-missing-data-encoder-cross-channel-image-completion-with-hide-and-seek-adversarial-network/">The Missing Data Encoder: Cross-Channel Image Completion with Hide-and-Seek Adversarial Network</a></h5><span class="papers-author-page"><p>Arnaud Dapogny, Matthieu Cord, Patrick Perez</p><p>10688-10695</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6696/6696-13-9925-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10696-spatio-temporal-deformable-convolution-for-compressed-video-quality-enhancement/">Spatio-Temporal Deformable Convolution for Compressed Video Quality Enhancement</a></h5><span class="papers-author-page"><p>Jianing Deng, Li Wang, Shiliang Pu, Cheng Zhuo</p><p>10696-10703</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6697/6697-13-9926-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10704-zero-shot-learning-with-the-isoperimetric-loss/">Zero Shot Learning with the Isoperimetric Loss</a></h5><span class="papers-author-page"><p>Shay Deutsch, Andrea Bertozzi, Stefano Soatto</p><p>10704-10712</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6698/6698-13-9927-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10713-every-frame-counts-joint-learning-of-video-segmentation-and-optical-flow/">Every Frame Counts: Joint Learning of Video Segmentation and Optical Flow</a></h5><span class="papers-author-page"><p>Mingyu Ding, Zhe Wang, Bolei Zhou, Jianping Shi, Zhiwu Lu, Ping Luo</p><p>10713-10720</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6699/6699-13-9928-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10721-cycle-cnn-for-colorization-towards-real-monochrome-color-camera-systems/">Cycle-CNN for Colorization towards Real Monochrome-Color Camera Systems</a></h5><span class="papers-author-page"><p>Xuan Dong, Weixin Li, Xiaojie Wang, Yunhong Wang</p><p>10721-10728</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6700/6700-13-9929-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10729-fd-gan-generative-adversarial-networks-with-fusion-discriminator-for-single-image-dehazing/">FD-GAN: Generative Adversarial Networks with Fusion-Discriminator for Single Image Dehazing</a></h5><span class="papers-author-page"><p>Yu Dong, Yihao Liu, He Zhang, Shifeng Chen, Yu Qiao</p><p>10729-10736</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6701/6701-13-9930-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10737-visual-relationship-detection-with-low-rank-non-negative-tensor-decomposition/">Visual Relationship Detection with Low Rank Non-Negative Tensor Decomposition</a></h5><span class="papers-author-page"><p>Mohammed Haroon Dupty, Zhen Zhang, Wee Sun Lee</p><p>10737-10744</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6702/6702-13-9931-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10745-subspace-capsule-network/">SubSpace Capsule Network</a></h5><span class="papers-author-page"><p>Marzieh Edraki, Nazanin Rahnavard, Mubarak Shah</p><p>10745-10753</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6703/6703-13-9932-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10754-person-tube-retrieval-via-language-description/">Person Tube Retrieval via Language Description</a></h5><span class="papers-author-page"><p>Hehe Fan, Yi Yang</p><p>10754-10761</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6704/6704-13-9933-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10762-cian-cross-image-affinity-net-for-weakly-supervised-semantic-segmentation/">CIAN: Cross-Image Affinity Net for Weakly Supervised Semantic Segmentation</a></h5><span class="papers-author-page"><p>Junsong Fan, Zhaoxiang Zhang, Tieniu Tan, Chunfeng Song, Jun Xiao</p><p>10762-10769</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6705/6705-13-9934-1-10-20200522.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10443-ultrafast-photorealistic-style-transfer-via-neural-architecture-search/">Ultrafast Photorealistic Style Transfer via Neural Architecture Search</a></h5><span class="papers-author-page"><p>Jie An, Haoyi Xiong, Jun Huan, Jiebo Luo</p><p>10443-10450</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6614/6614-13-9842-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10451-psynet-self-supervised-approach-to-object-localization-using-point-symmetric-transformation/">PsyNet: Self-Supervised Approach to Object Localization Using Point Symmetric Transformation</a></h5><span class="papers-author-page"><p>Kyungjune Baek, Minhyun Lee, Hyunjung Shim</p><p>10451-10459</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6615/6615-13-9843-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10460-detecting-human-object-interactions-via-functional-generalization/">Detecting Human-Object Interactions via Functional Generalization</a></h5><span class="papers-author-page"><p>Ankan Bansal, Sai Saketh Rambhatla, Abhinav Shrivastava, Rama Chellappa</p><p>10460-10469</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6616/6616-13-9844-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10470-incremental-multi-domain-learning-with-network-latent-tensor-factorization/">Incremental Multi-Domain Learning with Network Latent Tensor Factorization</a></h5><span class="papers-author-page"><p>Adrian Bulat, Jean Kossaifi, Georgios Tzimiropoulos, Maja Pantic</p><p>10470-10477</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6617/6617-13-9845-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10478-monocular-3d-object-detection-with-decoupled-structured-polygon-estimation-and-height-guided-depth-estimation/">Monocular 3D Object Detection with Decoupled Structured Polygon Estimation and Height-Guided Depth Estimation</a></h5><span class="papers-author-page"><p>Yingjie Cai, Buyu Li, Zeyu Jiao, Hongsheng Li, Xingyu Zeng, Xiaogang Wang</p><p>10478-10485</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6618/6618-13-9846-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10486-auto-gan-self-supervised-collaborative-learning-for-medical-image-synthesis/">Auto-GAN: Self-Supervised Collaborative Learning for Medical Image Synthesis</a></h5><span class="papers-author-page"><p>Bing Cao, Han Zhang, Nannan Wang, Xinbo Gao, Dinggang Shen</p><p>10486-10493</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6619/6619-13-9847-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10494-feature-deformation-meta-networks-in-image-captioning-of-novel-objects/">Feature Deformation Meta-Networks in Image Captioning of Novel Objects</a></h5><span class="papers-author-page"><p>Tingjia Cao, Ke Han, Xiaomei Wang, Lin Ma, Yanwei Fu, Yu-Gang Jiang, Xiangyang Xue</p><p>10494-10501</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6620/6620-13-9848-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10502-general-partial-label-learning-via-dual-bipartite-graph-autoencoder/">General Partial Label Learning via Dual Bipartite Graph Autoencoder</a></h5><span class="papers-author-page"><p>Brian Chen, Bo Wu, Alireza Zareian, Hanwang Zhang, Shih-Fu Chang</p><p>10502-10509</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6621/6621-13-9849-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10510-learning-deep-relations-to-promote-saliency-detection/">Learning Deep Relations to Promote Saliency Detection</a></h5><span class="papers-author-page"><p>Changrui Chen, Xin Sun, Yang Hua, Junyu Dong, Hongwei Xv</p><p>10510-10517</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6622/6622-13-9850-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10518-hierarchical-online-instance-matching-for-person-search/">Hierarchical Online Instance Matching for Person Search</a></h5><span class="papers-author-page"><p>Di Chen, Shanshan Zhang, Wanli Ouyang, Jian Yang, Bernt Schiele</p><p>10518-10525</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6623/6623-13-9851-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10526-binarized-neural-architecture-search/">Binarized Neural Architecture Search</a></h5><span class="papers-author-page"><p>Hanlin Chen, Li'an Zhuo, Baochang Zhang, Xiawu Zheng, Jianzhuang Liu, David Doermann, Rongrong Ji</p><p>10526-10533</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6624/6624-13-9852-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10534-end-to-end-learning-of-object-motion-estimation-from-retinal-events-for-event-based-object-tracking/">End-to-End Learning of Object Motion Estimation from Retinal Events for Event-Based Object Tracking</a></h5><span class="papers-author-page"><p>Haosheng Chen, David Suter, Qiangqiang Wu, Hanzi Wang</p><p>10534-10541</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6625/6625-13-9853-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10542-zero-shot-ingredient-recognition-by-multi-relational-graph-convolutional-network/">Zero-Shot Ingredient Recognition by Multi-Relational Graph Convolutional Network</a></h5><span class="papers-author-page"><p>Jingjing Chen, Liangming Pan, Zhipeng Wei, Xiang Wang, Chong-Wah Ngo, Tat-Seng Chua</p><p>10542-10550</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6626/6626-13-9854-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10551-rethinking-the-bottom-up-framework-for-query-based-video-localization/">Rethinking the Bottom-Up Framework for Query-Based Video Localization</a></h5><span class="papers-author-page"><p>Long Chen, Chujie Lu, Siliang Tang, Jun Xiao, Dong Zhang, Chilie Tan, Xiaolin Li</p><p>10551-10558</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6627/6627-13-9855-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10559-diversity-transfer-network-for-few-shot-learning/">Diversity Transfer Network for Few-Shot Learning</a></h5><span class="papers-author-page"><p>Mengting Chen, Yuxin Fang, Xinggang Wang, Heng Luo, Yifeng Geng, Xinyu Zhang, Chang Huang, Wenyu Liu, Bo Wang</p><p>10559-10566</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6628/6628-13-9856-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10567-structure-aware-feature-fusion-for-unsupervised-domain-adaptation/">Structure-Aware Feature Fusion for Unsupervised Domain Adaptation</a></h5><span class="papers-author-page"><p>Qingchao Chen, Yang Liu</p><p>10567-10574</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6629/6629-13-9857-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10575-knowledge-graph-transfer-network-for-few-shot-recognition/">Knowledge Graph Transfer Network for Few-Shot Recognition</a></h5><span class="papers-author-page"><p>Riquan Chen, Tianshui Chen, Xiaolu Hui, Hefeng Wu, Guanbin Li, Liang Lin</p><p>10575-10582</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6630/6630-13-9858-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10583-expressing-objects-just-like-words-recurrent-visual-embedding-for-image-text-matching/">Expressing Objects Just Like Words: Recurrent Visual Embedding for Image-Text Matching</a></h5><span class="papers-author-page"><p>Tianlang Chen, Jiebo Luo</p><p>10583-10590</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6631/6631-13-9859-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10591-frame-guided-region-aligned-representation-for-video-person-re-identification/">Frame-Guided Region-Aligned Representation for Video Person Re-Identification</a></h5><span class="papers-author-page"><p>Zengqun Chen, Zhiheng Zhou, Junchu Huang, Pengyu Zhang, Bo Li</p><p>10591-10598</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6632/6632-13-9860-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10599-global-context-aware-progressive-aggregation-network-for-salient-object-detection/">Global Context-Aware Progressive Aggregation Network for Salient Object Detection</a></h5><span class="papers-author-page"><p>Zuyao Chen, Qianqian Xu, Runmin Cong, Qingming Huang</p><p>10599-10606</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6633/6633-13-9861-1-10-20200520.pdf">PDF</a></li><li class="paper-wrap"><h5><a href="https://aaai.org/papers/10435-learning-end-to-end-scene-flow-by-distilling-single-tasks-knowledge/">Learning End-to-End Scene Flow by Distilling Single Tasks Knowledge</a></h5><span class="papers-author-page"><p>Filippo Aleotti, Matteo Poggi, Fabio Tosi, Stefano Mattoccia</p><p>10435-10442</p></span><a class="wp-block-button" target="_blank" href="https://cdn.aaai.org/ojs/6613/6613-13-9841-1-10-20200520.pdf">PDF</a></li></ul></div></main><aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar" id="genesis-sidebar-primary"><h2 class="genesis-sidebar-title screen-reader-text">Primary Sidebar</h2></aside></div></div><footer class="site-footer"><div class="wrap"></div></footer><div class="proceedings-wrap"></div><!--googleoff: all-->
<script>
$(document).ready(function(){
	$('#close-icon').click(function(e) {
		e.preventDefault();
		$(".ubermenu-wrap").animate({width:'toggle'},350);
		$(this).toggleClass("dashicons-no-alt");
		$(this).toggleClass("dashicons-menu-alt3");
	});
	var currentUrl = window.location.href;
    if (currentUrl != 'https://aaai.org/') {
		if ($(window).width() > 959) {
        	$('.ubermenu-wrap').hide();
		}
    }
	$(window).resize(function() {
		if ($(window).width() > 959) {
			$('.ubermenu-wrap').hide();
		} else {
			$('.ubermenu-wrap').show();
		}
	});
});
</script>

	<script type="text/javascript">
		function genesisBlocksShare( url, title, w, h ){
			var left = ( window.innerWidth / 2 )-( w / 2 );
			var top  = ( window.innerHeight / 2 )-( h / 2 );
			return window.open(url, title, 'toolbar=no, location=no, directories=no, status=no, menubar=no, scrollbars=no, resizable=no, copyhistory=no, width=600, height=600, top='+top+', left='+left);
		}
	</script>
	<style type="text/css" media="screen"></style><script id="essential-blocks-blocks-localize-js-extra">
var eb_conditional_localize = [];
var EssentialBlocksLocalize = {"eb_plugins_url":"https:\/\/aaai.org\/wp-content\/plugins\/essential-blocks\/","image_url":"https:\/\/aaai.org\/wp-content\/plugins\/essential-blocks\/assets\/images","eb_wp_version":"6.7","eb_version":"5.0.0","eb_admin_url":"https:\/\/aaai.org\/wp-admin\/","rest_rootURL":"https:\/\/aaai.org\/wp-json\/","ajax_url":"https:\/\/aaai.org\/wp-admin\/admin-ajax.php","nft_nonce":"26f33cbdd3","post_grid_pagination_nonce":"91a475d4f0","placeholder_image":"https:\/\/aaai.org\/wp-content\/plugins\/essential-blocks\/assets\/images\/placeholder.png","is_pro_active":"false","upgrade_pro_url":"https:\/\/essential-blocks.com\/upgrade","responsiveBreakpoints":{"tablet":1024,"mobile":767}};
</script>
<script src="https://aaai.org/wp-content/plugins/essential-blocks/assets/js/eb-blocks-localize.js?ver=31d6cfe0d16ae931b73c" id="essential-blocks-blocks-localize-js"></script>
<script src="https://aaai.org/wp-content/plugins/cookie-law-info-controls/public/js/cookie-law-info-controls-public.js?ver=1.0.0" id="cookie-law-info-controls-js"></script>
<script src="https://aaai.org/wp-content/plugins/genesis-blocks/dist/assets/js/dismiss.js?ver=1738785528" id="genesis-blocks-dismiss-js-js"></script>
<script src="https://aaai.org/wp-includes/js/hoverIntent.min.js?ver=1.10.2" id="hoverIntent-js"></script>
<script src="https://aaai.org/wp-content/themes/genesis/lib/js/menu/superfish.min.js?ver=1.7.10" id="superfish-js"></script>
<script src="https://aaai.org/wp-content/themes/genesis/lib/js/menu/superfish.args.min.js?ver=3.5.0" id="superfish-args-js"></script>
<script src="https://aaai.org/wp-content/themes/genesis/lib/js/skip-links.min.js?ver=3.5.0" id="skip-links-js"></script>
<script id="genesis-sample-responsive-menu-js-extra">
var genesis_responsive_menu = {"mainMenu":"Menu","menuIconClass":"dashicons-before dashicons-menu","subMenu":"Submenu","subMenuIconClass":"dashicons-before dashicons-arrow-down-alt2","menuClasses":{"others":[".nav-primary"]}};
</script>
<script src="https://aaai.org/wp-content/themes/genesis/lib/js/menu/responsive-menus.min.js?ver=1.1.3" id="genesis-sample-responsive-menu-js"></script>
<script id="ubermenu-js-extra">
var ubermenu_data = {"remove_conflicts":"on","reposition_on_load":"off","intent_delay":"300","intent_interval":"100","intent_threshold":"7","scrollto_offset":"50","scrollto_duration":"1000","responsive_breakpoint":"959","accessible":"on","retractor_display_strategy":"responsive","touch_off_close":"on","submenu_indicator_close_mobile":"on","collapse_after_scroll":"on","v":"3.7.8","configurations":["main"],"ajax_url":"https:\/\/aaai.org\/wp-admin\/admin-ajax.php","plugin_url":"https:\/\/aaai.org\/wp-content\/plugins\/ubermenu\/","disable_mobile":"off","prefix_boost":"","use_core_svgs":"off","aria_role_navigation":"off","aria_nav_label":"off","aria_expanded":"off","aria_hidden":"off","aria_controls":"","aria_responsive_toggle":"off","icon_tag":"i","esc_close_mobile":"on","theme_locations":{"primary":"Header Menu","secondary":"Footer Menu"}};
</script>
<script src="https://aaai.org/wp-content/plugins/ubermenu/assets/js/ubermenu.min.js?ver=3.7.8" id="ubermenu-js"></script>
</body></html>
